{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OghyhHyXsVi"
   },
   "source": [
    "# Problemas\n",
    "\n",
    "Nesta prática iremos usar tudo que aprendemos durante o módulo.\n",
    "Logo, **seu objetivo é determinar e implementar um modelo para cada problema.**\n",
    "\n",
    "Lembre-se de definir:\n",
    "\n",
    "1. o Dataloader, tratando a forma de ler as imagens de cada dataset, experimentando transformações diferentes (resize, crop, flips e etc.)\n",
    "1. uma arquitetura (tentem usar tanto arquiteturas existentes como propor novas usando camadas de convolução, pooling, e densas),\n",
    "1. uma função de custo\n",
    "1. um algoritmo de otimização (agora, como os problemas são maiores, será possível notar mais claramente a diferença entre diferentes algoritmos).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "uY-xoMGezgnu"
   },
   "outputs": [],
   "source": [
    "# %load_ext nbproxy\n",
    "# %pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bJkH8GxmXzuC"
   },
   "outputs": [],
   "source": [
    "import time, os, sys, numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from torch import optim\n",
    "from torchsummary import summary\n",
    "\n",
    "# Test if GPU is avaliable, if not, use cpu instead\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n = torch.cuda.device_count()\n",
    "devices_ids= list(range(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "udlf9BxBX3Vx"
   },
   "outputs": [],
   "source": [
    "# funções básicas\n",
    "\n",
    "# Função usada para calcular acurácia\n",
    "def evaluate_accuracy(data_iter, net, loss):\n",
    "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
    "\n",
    "    acc_sum, n, l = torch.Tensor([0]), 0, 0\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "      for X, y in data_iter:\n",
    "          #y = y.astype('float32')\n",
    "          X, y = X.to(device), y.to(device)\n",
    "          y_hat = net(X)\n",
    "          l += loss(y_hat, y).sum()\n",
    "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "          n += y.size()[0]\n",
    "\n",
    "    return acc_sum.item() / n, l.item() / len(data_iter)\n",
    "\n",
    "# Função usada no treinamento e validação da rede\n",
    "def train_validate(net, train_iter, test_iter, batch_size, trainer, loss,\n",
    "                   num_epochs):\n",
    "    print('training on', device)\n",
    "    for epoch in range(num_epochs):\n",
    "        net.train()\n",
    "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            y_hat = net(X)\n",
    "            trainer.zero_grad()\n",
    "            l = loss(y_hat, y).sum()\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_l_sum += l.item()\n",
    "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
    "            n += y.size()[0]\n",
    "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
    "        print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
    "              'test acc %.3f, time %.1f sec'\n",
    "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n",
    "                 test_acc, time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Su-ITUO4eJ7p"
   },
   "source": [
    "\n",
    "## Problema 1 - Exemplo\n",
    "\n",
    "Neste problema, classificaremos imagens histólogica do dataset [*Colorectal Histology*](https://www.kaggle.com/kmader/colorectal-histology-mnist).\n",
    "Neste caso, vamos receber imagens com tamanho de $150\\times 150$ pixels e classificá-las entre 8 classes:\n",
    "\n",
    "1. tumor\n",
    "1. stroma\n",
    "1. complex\n",
    "1. lympho\n",
    "1. debris\n",
    "1. mucosa\n",
    "1. adipose\n",
    "1. empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_d2-W5a0Kap",
    "outputId": "93261cf7-5acd-4fa9-a110-9235a70ebfdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-01 19:58:29--  https://www.dropbox.com/s/k0f6vxyhcr6gh1r/Kather_texture_2016_image_tiles_5000.zip\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 2620:100:601d:18::a27d:512, 162.125.5.18\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|2620:100:601d:18::a27d:512|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.dropbox.com/scl/fi/eyds9w2eqfkq7rx86z4oj/Kather_texture_2016_image_tiles_5000.zip?rlkey=w9zrj0hx7dflh7913i4ds0fi7 [following]\n",
      "--2025-07-01 19:58:30--  https://www.dropbox.com/scl/fi/eyds9w2eqfkq7rx86z4oj/Kather_texture_2016_image_tiles_5000.zip?rlkey=w9zrj0hx7dflh7913i4ds0fi7\n",
      "Reusing existing connection to [www.dropbox.com]:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com/cd/0/inline/CssdYp4DeipJfq4ar3A-VDTVakyBqCoZQAaRkaYVxEGIXaI-p6URZ9rPTiRkx8VwdQT3gSgZ29GnfxIeJdxEAzpyg97_BQYbRz9EbCYuHlOOsdk4vVIO_h22-4WqH1dZgCI/file# [following]\n",
      "--2025-07-01 19:58:30--  https://ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com/cd/0/inline/CssdYp4DeipJfq4ar3A-VDTVakyBqCoZQAaRkaYVxEGIXaI-p6URZ9rPTiRkx8VwdQT3gSgZ29GnfxIeJdxEAzpyg97_BQYbRz9EbCYuHlOOsdk4vVIO_h22-4WqH1dZgCI/file\n",
      "Resolving ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com (ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com)... 2620:100:601d:15::a27d:50f, 162.125.5.15\n",
      "Connecting to ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com (ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com)|2620:100:601d:15::a27d:50f|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: /cd/0/inline2/CssMblohLVGk5Ar3BlMUdaeL9B7X19q9F0dIMwNK8lQER-WWKHFJPmzpXDf-ptVeQ1SJrHj8ii-PRtthlm4AT7-rPYpeniOJNDkcD0VNpXXVYuf4bga2KlK_PChBGKA5Dn_JW4IBnpdEq7hUcVXKePNqQlqp_sbZNzOrpqsgwRtbMEz9zz0T1ELsaBMbCkergTcgC2jdqvZhP-6evUkS04JvNG8g4LB9Z5stKIpPzWX4cRfv4yxc3hmP5MDVuG3o5_kjRVruKhq6inls19YgTSAiKfCT0h3WD5EuKi8eXzyN8uaADHkGpPCH8jChOaOG90-K3ZyLvJUQG80iZ0mTO2Fq79NAL7hhCzL1hjJ-Ttd-Eg/file [following]\n",
      "--2025-07-01 19:58:32--  https://ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com/cd/0/inline2/CssMblohLVGk5Ar3BlMUdaeL9B7X19q9F0dIMwNK8lQER-WWKHFJPmzpXDf-ptVeQ1SJrHj8ii-PRtthlm4AT7-rPYpeniOJNDkcD0VNpXXVYuf4bga2KlK_PChBGKA5Dn_JW4IBnpdEq7hUcVXKePNqQlqp_sbZNzOrpqsgwRtbMEz9zz0T1ELsaBMbCkergTcgC2jdqvZhP-6evUkS04JvNG8g4LB9Z5stKIpPzWX4cRfv4yxc3hmP5MDVuG3o5_kjRVruKhq6inls19YgTSAiKfCT0h3WD5EuKi8eXzyN8uaADHkGpPCH8jChOaOG90-K3ZyLvJUQG80iZ0mTO2Fq79NAL7hhCzL1hjJ-Ttd-Eg/file\n",
      "Reusing existing connection to [ucbc877f81b3abe7066d4f47493e.dl.dropboxusercontent.com]:443.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 258098431 (246M) [application/zip]\n",
      "Saving to: ‘Kather_texture_2016_image_tiles_5000.zip’\n",
      "\n",
      "Kather_texture_2016 100%[===================>] 246,14M  29,3MB/s    in 11s     \n",
      "\n",
      "2025-07-01 19:58:44 (22,8 MB/s) - ‘Kather_texture_2016_image_tiles_5000.zip’ saved [258098431/258098431]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/k0f6vxyhcr6gh1r/Kather_texture_2016_image_tiles_5000.zip\n",
    "!unzip -q Kather_texture_2016_image_tiles_5000.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "R7PqlFcLeG-h"
   },
   "outputs": [],
   "source": [
    "class HistologyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, train=False, calc_norm=True, has_norm=True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.calc_norm = calc_norm\n",
    "        self.has_norm = has_norm\n",
    "        self.le = {'tumor': 0, 'stroma': 1, 'complex': 2, 'lympho': 3,\n",
    "                   'debris': 4, 'mucosa': 5, 'adipose': 6, 'empty': 7} # dicionário para definir o label de cada classe\n",
    "        self.transform = transform\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        self.img_list, self.labels = self.read_images(root=self.root)\n",
    "\n",
    "    def read_images(self, root):\n",
    "        # Leitura das imagens do dataset\n",
    "        # para este caso, o dataset divide em pastas as imagens de cada classe correspondente\n",
    "        # portanto, vamos percorrer essas pastas, adicionando as primeiras 500 imagens para o conjunto de treino\n",
    "        # o restante das imagens (a partir da 500) são adicionadas na validação\n",
    "        # o label é definido de acordo com o nome da pasta pelo dicionário self.le definido acima\n",
    "        # por exemplo: a pasta 01_TUMOR vai ser correspondente ao self.le['tumor'], que é igual a 0\n",
    "        img_list, labels = [], []\n",
    "        if self.train is True:\n",
    "          for folder in os.listdir(self.root):\n",
    "            for num, img_name in enumerate(os.listdir(os.path.join(self.root, folder))):\n",
    "                if num < 500:\n",
    "                  img_list.append(os.path.join(self.root, folder, img_name))\n",
    "                  labels.append(self.le[folder.split('_')[1].lower()])\n",
    "        else:\n",
    "          for folder in os.listdir(os.path.join(self.root)):\n",
    "            for num, img_name in enumerate(os.listdir(os.path.join(self.root, folder))):\n",
    "                if num >= 500:\n",
    "                  img_list.append(os.path.join(self.root, folder, img_name))\n",
    "                  labels.append(self.le[folder.split('_')[1].lower()])\n",
    "\n",
    "        return img_list, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # retorna uma imagem para o treino/teste\n",
    "        if self.has_norm is True:\n",
    "            # normaliza a imagem se has_norm for setado como True\n",
    "            cur_img = self.normalize_image(self.transform(Image.open(self.img_list[item])))\n",
    "        else:\n",
    "            # apenas converte a imagem para tensor, sem normalizar\n",
    "            cur_img = self.transform(Image.open(self.img_list[item]))\n",
    "        cur_label = self.labels[item]\n",
    "        return cur_img, cur_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        # normaliza uma imagem\n",
    "        # se calc_norm for True, normaliza pela subtração da média dividida pelo desvio para cada canal da imagem\n",
    "        # se calc_norm for False, normaliza pelos valores pré-definidos de média e desvio padrão\n",
    "        if self.calc_norm is True:\n",
    "            for i in range(img.shape[0]):\n",
    "                mu = img[i, :, :].mean()\n",
    "                std = img[i, :, :].std()\n",
    "                img[i, :, :] = ((img[i, :, :] - mu) / std)\n",
    "        else:\n",
    "            img = torchvision.transforms.functional.normalize(img,\n",
    "                                                mean=torch.Tensor([0.485, 0.456, 0.406]),\n",
    "                                                std=torch.Tensor([0.229, 0.224, 0.225]))\n",
    "        return img\n",
    "\n",
    "\n",
    "def load_data(dataset, root, batch_size, resize=None):\n",
    "    # o transformer define a sequência de transformações que serão aplicadas na imagem\n",
    "    # neste caso, a sequência é um redimensionamento da imagem (caso a variável resize seja definida)\n",
    "    # seguido de uma transformação para tensor\n",
    "    # várias outras transformações estão disponíveis no Pytorch, como crops, flips, espelhamento e etc.\n",
    "    transformer = []\n",
    "    if resize is not None:\n",
    "        transformer += [torchvision.transforms.Resize(size=(resize,resize))]\n",
    "    transformer += [torchvision.transforms.ToTensor()]\n",
    "    transformer = torchvision.transforms.Compose(transformer)\n",
    "\n",
    "    train = dataset(root=root, transform=transformer, train=True) #obtem dataset de treino\n",
    "    test = dataset(root=root, transform=transformer, train=False) #obtem dataset de validação\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train,\n",
    "                                  batch_size, shuffle=True,\n",
    "                                  num_workers=num_workers) # criação do dataloader de treino\n",
    "    test_iter = torch.utils.data.DataLoader(test,\n",
    "                                 batch_size, shuffle=False,\n",
    "                                 num_workers=num_workers) # criação do dataloader de teste\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# carregamento do dado\n",
    "batch_size = 64\n",
    "train_iter, test_iter = load_data(HistologyDataset, 'Kather_texture_2016_image_tiles_5000', batch_size, resize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MajgTJrh0PsR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n",
      "epoch 1, train loss 1.4825, train acc 0.434, test loss 0.8921, test acc 0.666, time 6.5 sec\n",
      "epoch 2, train loss 0.8576, train acc 0.658, test loss 0.8280, test acc 0.693, time 5.7 sec\n",
      "epoch 3, train loss 0.6944, train acc 0.730, test loss 0.6709, test acc 0.784, time 5.6 sec\n",
      "epoch 4, train loss 0.5273, train acc 0.808, test loss 0.6226, test acc 0.794, time 5.6 sec\n",
      "epoch 5, train loss 0.4405, train acc 0.846, test loss 0.5751, test acc 0.820, time 5.6 sec\n",
      "epoch 6, train loss 0.3616, train acc 0.864, test loss 0.5897, test acc 0.822, time 5.7 sec\n",
      "epoch 7, train loss 0.2253, train acc 0.921, test loss 0.7089, test acc 0.766, time 5.7 sec\n",
      "epoch 8, train loss 0.1475, train acc 0.956, test loss 0.6762, test acc 0.816, time 5.6 sec\n",
      "epoch 9, train loss 0.1307, train acc 0.957, test loss 0.6948, test acc 0.811, time 5.7 sec\n",
      "epoch 10, train loss 0.1743, train acc 0.942, test loss 0.7166, test acc 0.825, time 5.6 sec\n",
      "epoch 11, train loss 0.0699, train acc 0.981, test loss 0.7652, test acc 0.814, time 5.5 sec\n",
      "epoch 12, train loss 0.0644, train acc 0.981, test loss 0.8392, test acc 0.808, time 5.4 sec\n",
      "epoch 13, train loss 0.0646, train acc 0.979, test loss 0.8384, test acc 0.832, time 5.3 sec\n",
      "epoch 14, train loss 0.0573, train acc 0.983, test loss 0.9364, test acc 0.814, time 5.7 sec\n",
      "epoch 15, train loss 0.0766, train acc 0.977, test loss 1.0408, test acc 0.804, time 5.7 sec\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTE AQUI SUA REDE E DEFINIÇÃO DE LOSS E OTIMIZADOR\n",
    "\n",
    "# experimente criar redes do zero com o conhecimento adquirido no curso até agora\n",
    "# experimente também replicar redes já estabelecidas (alexnet, lenet, vgg e etc)\n",
    "# experimente também utilizar as redes pré-treinadas já implementadas no torchvision\n",
    "# para o caso de rede pré-treinada, lembre-se de modificar a saída da rede para o número de classes do problema\n",
    "\n",
    "class HistologyCNN(nn.Module):\n",
    "    def __init__(self, num_classes=8):\n",
    "        super(HistologyCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 18 * 18, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "num_epochs = 15\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "wd_lambda = 0.0001\n",
    "\n",
    "model = HistologyCNN(num_classes=8).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=wd_lambda)\n",
    "\n",
    "\n",
    "# treinamento e validação\n",
    "train_validate(model, train_iter, test_iter, batch_size, optimizer, loss, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03GGRGweYCZm"
   },
   "source": [
    "## Problema 2\n",
    "\n",
    "Neste problema, classificaremos imagens de sensoriamento remoto de plantações de café do dataset público [Brazilian Coffee Scenes](http://www.patreo.dcc.ufmg.br/2017/11/12/brazilian-coffee-scenes-dataset/).\n",
    "Neste caso, , vamos receber imagens de $64\\times 64$ pixels e classificá-las entre duas classes:\n",
    "\n",
    "1. café, e\n",
    "2. não café."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Pb9SyKmrY_c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-01 21:16:29--  http://www.patreo.dcc.ufmg.br/wp-content/uploads/2017/11/brazilian_coffee_dataset.zip\n",
      "Resolving www.patreo.dcc.ufmg.br (www.patreo.dcc.ufmg.br)... 150.164.144.15\n",
      "Connecting to www.patreo.dcc.ufmg.br (www.patreo.dcc.ufmg.br)|150.164.144.15|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4796290 (4,6M) [application/zip]\n",
      "Saving to: ‘brazilian_coffee_dataset.zip’\n",
      "\n",
      "brazilian_coffee_da 100%[===================>]   4,57M  4,99MB/s    in 0,9s    \n",
      "\n",
      "2025-07-01 21:16:30 (4,99 MB/s) - ‘brazilian_coffee_dataset.zip’ saved [4796290/4796290]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baixando o dataset\n",
    "!wget http://www.patreo.dcc.ufmg.br/wp-content/uploads/2017/11/brazilian_coffee_dataset.zip\n",
    "!unzip -q brazilian_coffee_dataset.zip\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znSdowm6YAtq"
   },
   "outputs": [],
   "source": [
    "class CoffeeDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, train=False, calc_norm=True, has_norm=True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.calc_norm = calc_norm\n",
    "        self.has_norm = has_norm\n",
    "        self.load_images()\n",
    "        self.transform = transform\n",
    "\n",
    "    def load_images(self):\n",
    "        self.img_list, self.labels = self.read_images(root=self.root)\n",
    "\n",
    "    def read_images(self, root):\n",
    "        # para este dataset, existem 5 pastas (fold1, fold2, ..., fold5) com as imagens\n",
    "        # e existem 5 arquivos txts (fold1.txt, fold2.txt, ..., fold5.txt) com o nome das imagens correspondentes\n",
    "        # nos arquivos txts, cada linha representa uma imagem seguindo o formato {classe}.{nome da img}\n",
    "        # sendo classe igual a coffee ou noncoffee (0 ou 1)\n",
    "        # tratamos o nome das imagens de acordo com cada linha do arquivo (não esquecendo de adicionar o .jpg)\n",
    "        # convertemos o label em 0 ou 1 dependendo da classe\n",
    "        # Vamos utilizar o fold 1 como validação e o restante dos folds como treino\n",
    "        img_list, labels = [], []\n",
    "        if self.train is True:\n",
    "          for i in range(1,5):\n",
    "            data_file = open(os.path.join(root, 'fold' + str(i+1) + '.txt'), \"r\")  # arquivo com nome das imagens\n",
    "            data_list = [i.replace('\\n', '') for i in data_file.readlines()]\n",
    "            for row in data_list:\n",
    "                img_name = '.'.join(row.split('.')[1:])\n",
    "                img_list.append(os.path.join(root, 'fold' + str(i+1), img_name + '.jpg'))\n",
    "                labels.append(0 if row.split('.')[0] == 'coffee' else 1)\n",
    "        else:\n",
    "            data_file = open(os.path.join(root, 'fold1.txt'), \"r\")  # arquivo com nome das imagens\n",
    "            data_list = [i.replace('\\n', '') for i in data_file.readlines()]\n",
    "            for row in data_list:\n",
    "                img_name = '.'.join(row.split('.')[1:])\n",
    "                img_list.append(os.path.join(root, 'fold1', img_name + '.jpg'))\n",
    "                labels.append(0 if row.split('.')[0] == 'coffee' else 1)\n",
    "\n",
    "        return img_list, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # retorna uma imagem para o treino/teste\n",
    "        if self.has_norm is True:\n",
    "            # normaliza a imagem se has_norm for setado como True\n",
    "            cur_img = self.normalize_image(self.transform(Image.open(self.img_list[item])))\n",
    "        else:\n",
    "            # apenas converte a imagem para tensor, sem normalizar\n",
    "            cur_img = self.transform(Image.open(self.img_list[item]))\n",
    "        cur_label = self.labels[item]\n",
    "        return cur_img, cur_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        # normaliza uma imagem\n",
    "        # se calc_norm for True, normaliza pela subtração da média dividida pelo desvio para cada canal da imagem\n",
    "        # se calc_norm for False, normaliza pelos valores pré-definidos de média e desvio padrão\n",
    "        if self.calc_norm is True:\n",
    "            for i in range(img.shape[0]):\n",
    "                mu = img[i, :, :].mean()\n",
    "                std = img[i, :, :].std()\n",
    "                img[i, :, :] = ((img[i, :, :] - mu) / std)\n",
    "        else:\n",
    "            img = torchvision.transforms.functional.normalize(img,\n",
    "                                                mean=torch.Tensor([0.485, 0.456, 0.406]),\n",
    "                                                std=torch.Tensor([0.229, 0.224, 0.225]))\n",
    "        return img\n",
    "\n",
    "\n",
    "def load_data(dataset, root, batch_size, resize=None):\n",
    "    # o transformer define a sequência de transformações que serão aplicadas na imagem\n",
    "    # neste caso, a sequência é um redimensionamento da imagem (caso a variável resize seja definida)\n",
    "    # seguido de uma transformação para tensor\n",
    "    # várias outras transformações estão disponíveis no Pytorch, como crops, flips, espelhamento e etc.\n",
    "    transformer = []\n",
    "    if resize is not None:\n",
    "        transformer += [torchvision.transforms.Resize(size=(resize,resize))]\n",
    "    transformer += [torchvision.transforms.ToTensor()]\n",
    "    transformer = torchvision.transforms.Compose(transformer)\n",
    "\n",
    "    train = dataset(root=root, transform=transformer, train=True) #obtem dataset de treino\n",
    "    test = dataset(root=root, transform=transformer, train=False) #obtem dataset de validação\n",
    "    num_workers = 0 if sys.platform.startswith('win32') else 4\n",
    "\n",
    "    train_iter = torch.utils.data.DataLoader(train,\n",
    "                                  batch_size, shuffle=True,\n",
    "                                  num_workers=num_workers) # criação do dataloader de treino\n",
    "    test_iter = torch.utils.data.DataLoader(test,\n",
    "                                 batch_size, shuffle=False,\n",
    "                                 num_workers=num_workers) # criação do dataloader de teste\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# carregamento do dado\n",
    "batch_size = 64\n",
    "train_iter, test_iter = load_data(CoffeeDataset, 'brazilian_coffee_scenes', batch_size, resize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-4J_GJRXLMq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on cuda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train loss 1.5347, train acc 0.500, test loss 0.6231, test acc 0.500, time 1.1 sec\n",
      "epoch 2, train loss 0.6213, train acc 0.500, test loss 0.4585, test acc 0.500, time 1.1 sec\n",
      "epoch 3, train loss 0.5214, train acc 0.500, test loss 0.4483, test acc 0.500, time 1.1 sec\n",
      "epoch 4, train loss 0.4827, train acc 0.500, test loss 0.3604, test acc 0.500, time 1.0 sec\n",
      "epoch 5, train loss 0.4722, train acc 0.500, test loss 0.3775, test acc 0.500, time 1.0 sec\n",
      "epoch 6, train loss 0.4274, train acc 0.500, test loss 0.3571, test acc 0.500, time 1.0 sec\n",
      "epoch 7, train loss 0.4127, train acc 0.500, test loss 0.3626, test acc 0.500, time 1.1 sec\n",
      "epoch 8, train loss 0.4013, train acc 0.500, test loss 0.3235, test acc 0.500, time 1.1 sec\n",
      "epoch 9, train loss 0.4120, train acc 0.500, test loss 0.4100, test acc 0.500, time 1.0 sec\n",
      "epoch 10, train loss 0.4099, train acc 0.500, test loss 0.3396, test acc 0.500, time 1.0 sec\n",
      "epoch 11, train loss 0.3787, train acc 0.500, test loss 0.3559, test acc 0.500, time 1.0 sec\n",
      "epoch 12, train loss 0.3508, train acc 0.500, test loss 0.3202, test acc 0.500, time 1.0 sec\n",
      "epoch 13, train loss 0.3565, train acc 0.500, test loss 0.3065, test acc 0.500, time 1.0 sec\n",
      "epoch 14, train loss 0.3673, train acc 0.500, test loss 0.3350, test acc 0.500, time 1.0 sec\n",
      "epoch 15, train loss 0.3658, train acc 0.500, test loss 0.3265, test acc 0.500, time 1.0 sec\n",
      "epoch 16, train loss 0.3419, train acc 0.500, test loss 0.3217, test acc 0.500, time 1.0 sec\n",
      "epoch 17, train loss 0.3315, train acc 0.500, test loss 0.3597, test acc 0.500, time 1.1 sec\n",
      "epoch 18, train loss 0.3256, train acc 0.500, test loss 0.3018, test acc 0.500, time 1.0 sec\n",
      "epoch 19, train loss 0.3418, train acc 0.500, test loss 0.3995, test acc 0.500, time 1.1 sec\n",
      "epoch 20, train loss 0.3364, train acc 0.500, test loss 0.3709, test acc 0.500, time 1.0 sec\n",
      "epoch 21, train loss 0.3225, train acc 0.500, test loss 0.4980, test acc 0.500, time 1.0 sec\n",
      "epoch 22, train loss 0.3192, train acc 0.500, test loss 0.2919, test acc 0.500, time 1.1 sec\n",
      "epoch 23, train loss 0.2912, train acc 0.500, test loss 0.3359, test acc 0.500, time 1.1 sec\n",
      "epoch 24, train loss 0.2980, train acc 0.500, test loss 0.2907, test acc 0.500, time 1.1 sec\n",
      "epoch 25, train loss 0.3118, train acc 0.500, test loss 0.3884, test acc 0.500, time 1.0 sec\n",
      "epoch 26, train loss 0.2968, train acc 0.500, test loss 0.3611, test acc 0.500, time 1.0 sec\n",
      "epoch 27, train loss 0.2874, train acc 0.500, test loss 0.2983, test acc 0.500, time 1.1 sec\n",
      "epoch 28, train loss 0.2987, train acc 0.500, test loss 0.3662, test acc 0.500, time 1.1 sec\n",
      "epoch 29, train loss 0.2859, train acc 0.500, test loss 0.3331, test acc 0.500, time 1.0 sec\n",
      "epoch 30, train loss 0.2781, train acc 0.500, test loss 0.2907, test acc 0.500, time 1.0 sec\n"
     ]
    }
   ],
   "source": [
    "# IMPLEMENTE AQUI SUA REDE E DEFINIÇÃO DE LOSS E OTIMIZADOR\n",
    "\n",
    "# experimente criar redes do zero com o conhecimento adquirido no curso até agora\n",
    "# experimente também replicar redes já estabelecidas (alexnet, lenet, vgg e etc)\n",
    "# experimente também utilizar as redes pré-treinadas já implementadas no torchvision\n",
    "# para o caso de rede pré-treinada, lembre-se de modificar a saída da rede para o número de classes do problema\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CoffeeCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CoffeeCNN, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool3 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.fc1 = nn.Linear(128 * 8 * 8, 256)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.bn1(self.conv1(x)), negative_slope=0.1)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn2(self.conv2(x)), negative_slope=0.1)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.leaky_relu(self.bn3(self.conv3(x)), negative_slope=0.1)\n",
    "        x = self.pool3(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = x.view(-1, 128 * 8 * 8)\n",
    "        x = F.leaky_relu(self.fc1(x), negative_slope=0.1)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "num_epochs = 30\n",
    "lr = 0.001\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "model = CoffeeCNN().to(device)\n",
    "loss = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "# treinamento e validação\n",
    "train_validate(model, train_iter, test_iter, batch_size, optimizer, lambda y_hat, y: loss(y_hat, y.float().view(-1, 1)), num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O59tc-hAdOm5"
   },
   "source": [
    "## Problema 3\n",
    "\n",
    "Neste problema, classificaremos imagens gerais de sensoriamento remoto do dataset público [UCMerced](http://weegee.vision.ucmerced.edu/datasets/landuse.html).\n",
    "Neste caso, vamos receber imagens de $256\\times 256$ pixels e classificá-las entre 21 classes:\n",
    "\n",
    "1. agricultural\n",
    "1. airplane\n",
    "1. baseballdiamond\n",
    "1. beach\n",
    "1. buildings\n",
    "1. chaparral\n",
    "1. denseresidential\n",
    "1. forest\n",
    "1. freeway\n",
    "1. golfcourse\n",
    "1. harbor\n",
    "1. intersection\n",
    "1. mediumresidential\n",
    "1. mobilehomepark\n",
    "1. overpass\n",
    "1. parkinglot\n",
    "1. river\n",
    "1. runway\n",
    "1. sparseresidential\n",
    "1. storagetanks\n",
    "1. tenniscourt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8IA0iam6pXbZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-07-01 22:26:26--  http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip\n",
      "Resolving weegee.vision.ucmerced.edu (weegee.vision.ucmerced.edu)... 169.236.184.65\n",
      "Connecting to weegee.vision.ucmerced.edu (weegee.vision.ucmerced.edu)|169.236.184.65|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 332468434 (317M) [application/zip]\n",
      "Saving to: ‘UCMerced_LandUse.zip’\n",
      "\n",
      "UCMerced_LandUse.zi 100%[===================>] 317,07M  13,2MB/s    in 82s     \n",
      "\n",
      "2025-07-01 22:27:50 (3,84 MB/s) - ‘UCMerced_LandUse.zip’ saved [332468434/332468434]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Baixando o dataset\n",
    "!wget http://weegee.vision.ucmerced.edu/datasets/UCMerced_LandUse.zip\n",
    "!unzip -q UCMerced_LandUse.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mlTW4qgWdM6y"
   },
   "outputs": [],
   "source": [
    "class UCMercedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, train=False, calc_norm=False, has_norm=True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.calc_norm = calc_norm\n",
    "        self.has_norm = has_norm\n",
    "        self.transform = transform\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        self.img_list, self.labels = self.read_images(root=self.root)\n",
    "\n",
    "    def read_images(self, root):\n",
    "        # IMPLEMENTE AQUI A LEITURA DAS IMAGENS\n",
    "\n",
    "        # para este dataset, as imagens estão dividas em pastas com o nome da classe\n",
    "        # uma sugestão é usar o enumerate do Python para percorrer essas pastas, atribuindo o valor do enumerate como label da classe\n",
    "        # outra opção é definir um dicionário com o label de cada classe como no exemplo do Colerectal histology\n",
    "        # para cada pasta, selecione as primeiras 80 imagens para o treino e o restante para o teste\n",
    "        img_list, labels = [], []\n",
    "\n",
    "        if self.train is True:\n",
    "            for folder in os.listdir(self.root):\n",
    "                for num, img_name in enumerate(os.listdir(os.path.join(self.root, folder))):\n",
    "                    if num < 80:\n",
    "                        img_list.append(os.path.join(self.root, folder, img_name))\n",
    "                        labels.append(folder)\n",
    "        else:\n",
    "            for folder in os.listdir(self.root):\n",
    "                for num, img_name in enumerate(os.listdir(os.path.join(self.root, folder))):\n",
    "                    if num >= 80:\n",
    "                        img_list.append(os.path.join(self.root, folder, img_name))\n",
    "                        labels.append(folder)\n",
    "\n",
    "        return img_list, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # IMPLEMENTE AQUI O RETORNO E TRATAMENTO DE CADA IMAGEM\n",
    "\n",
    "        # lembre-se de aplicar as transformações enviadas ao dataloader (principalmente o ToTensor)\n",
    "        \n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        # normaliza uma imagem\n",
    "        # se calc_norm for True, normaliza pela subtração da média dividida pelo desvio para cada canal da imagem\n",
    "        # se calc_norm for False, normaliza pelos valores pré-definidos de média e desvio padrão\n",
    "        if self.calc_norm is True:\n",
    "            for i in range(img.shape[0]):\n",
    "                mu = img[i, :, :].mean()\n",
    "                std = img[i, :, :].std()\n",
    "                img[i, :, :] = ((img[i, :, :] - mu) / std)\n",
    "        else:\n",
    "            img = torchvision.transforms.functional.normalize(img,\n",
    "                                                mean=torch.Tensor([0.485, 0.456, 0.406]),\n",
    "                                                std=torch.Tensor([0.229, 0.224, 0.225]))\n",
    "        return img\n",
    "\n",
    "\n",
    "def load_data(dataset, root, batch_size, resize=None):\n",
    "    # IMPLEMENTE AQUI A DEFINIÇÃO DAS TRANSFORMAÇÕES E DO DATALOADER\n",
    "\n",
    "    # o transformer define a sequência de transformações que serão aplicadas na imagem\n",
    "    # a principal para o nosso caso é o ToTensor, que converte a imagem no formato lido para um tensor\n",
    "    # experimente transformações diferentes, como crops e flips\n",
    "    # o resize pode ser necessário para datasets com imagems de tamanhos variados\n",
    "\n",
    "    # defina também o dataloader de treino e teste\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# carregamento do dado\n",
    "batch_size = 64\n",
    "train_iter, test_iter = load_data(UCMercedDataset, os.path.join('UCMerced_LandUse', 'Images'), batch_size, resize=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTCvFXC2q4Xa"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTE AQUI SUA REDE E DEFINIÇÃO DE LOSS E OTIMIZADOR\n",
    "\n",
    "# experimente criar redes do zero com o conhecimento adquirido no curso até agora\n",
    "# experimente também replicar redes já estabelecidas (alexnet, lenet, vgg e etc)\n",
    "# experimente também utilizar as redes pré-treinadas já implementadas no torchvision\n",
    "# para o caso de rede pré-treinada, lembre-se de modificar a saída da rede para o número de classes do problema\n",
    "\n",
    "# treinamento e validação\n",
    "train_validate(model, train_iter, test_iter, batch_size, optimizer, loss, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_26IvT9d5nF"
   },
   "source": [
    "## Problema 4\n",
    "\n",
    "Neste problema, classificaremos imagens genéricas de textura do dataset público [*Describable Textures Dataset*](http://www.robots.ox.ac.uk/~vgg/data/dtd/).\n",
    "Neste caso, vamos receber imagens com tamanho variado (de $300\\times 300$ pixels até $640\\times 640$) e classificá-las entre 47 classes:\n",
    "\n",
    "1.  banded\n",
    "1.  blotchy\n",
    "1.  braided\n",
    "1.  bubbly\n",
    "1.  bumpy\n",
    "1.  chequered\n",
    "1.  cobwebbed\n",
    "1.  cracked\n",
    "1.  crosshatched\n",
    "1.  crystalline\n",
    "1.  dotted\n",
    "1.  fibrous\n",
    "1.  flecked\n",
    "1.  freckled\n",
    "1.  frilly\n",
    "1.  gauzy\n",
    "1.  grid\n",
    "1.  grooved\n",
    "1.  honeycombed\n",
    "1.  interlaced\n",
    "1.  knitted\n",
    "1.  lacelike\n",
    "1.  lined\n",
    "1.  marbled\n",
    "1.  matted\n",
    "1.  meshed\n",
    "1.  paisley\n",
    "1.  perforated\n",
    "1.  pitted\n",
    "1.  pleated\n",
    "1.  polka-dotted\n",
    "1.  porous\n",
    "1.  potholed\n",
    "1.  scaly\n",
    "1.  smeared\n",
    "1.  spiralled\n",
    "1.  sprinkled\n",
    "1.  stained\n",
    "1.  stratified\n",
    "1.  striped\n",
    "1.  studded\n",
    "1.  swirly\n",
    "1.  veined\n",
    "1.  waffled\n",
    "1.  woven\n",
    "1.  wrinkled\n",
    "1.  zigzagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBeE2iv9yI-_"
   },
   "outputs": [],
   "source": [
    "# Download do dataset\n",
    "!wget http://www.robots.ox.ac.uk/~vgg/data/dtd/download/dtd-r1.0.1.tar.gz\n",
    "!tar -xzf dtd-r1.0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgWH1s5ndbhx"
   },
   "outputs": [],
   "source": [
    "class TextureDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transform, train=False, calc_norm=False, has_norm=True):\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.calc_norm = calc_norm\n",
    "        self.has_norm = has_norm\n",
    "        self.le = {'banded': 0, 'blotchy': 1, 'braided': 2, 'bubbly': 3, 'bumpy': 4,\n",
    "                   'chequered': 5, 'cobwebbed': 6, 'cracked': 7, 'crosshatched': 8,\n",
    "                   'crystalline': 9, 'dotted': 10, 'fibrous': 11, 'flecked': 12,\n",
    "                   'freckled': 13, 'frilly': 14, 'gauzy': 15, 'grid': 16, 'grooved': 17,\n",
    "                   'honeycombed': 18, 'interlaced': 19, 'knitted': 20, 'lacelike': 21, 'lined': 22,\n",
    "                   'marbled': 23, 'matted': 24, 'meshed': 25, 'paisley': 26, 'perforated': 27,\n",
    "                   'pitted': 28, 'pleated': 29, 'polka-dotted': 30, 'porous': 31, 'potholed': 32,\n",
    "                   'scaly': 33, 'smeared': 34, 'spiralled': 35, 'sprinkled': 36, 'stained': 37,\n",
    "                   'stratified': 38, 'striped': 39, 'studded': 40, 'swirly': 41, 'veined': 42,\n",
    "                   'waffled': 43, 'woven': 44, 'wrinkled': 45, 'zigzagged': 46} # dicionário definindo o label de cada classe\n",
    "        self.transform = transform\n",
    "        self.load_images()\n",
    "\n",
    "    def load_images(self):\n",
    "        self.img_list, self.labels = self.read_images(root=self.root)\n",
    "\n",
    "    def read_images(self, root):\n",
    "        # IMPLEMENTE AQUI A LEITURA DAS IMAGENS\n",
    "\n",
    "        # para este caso, na pasta images estão as imagens separadas por pastas relacionadas a classe\n",
    "        # na pasta label existem txts definindo a divisão das imagens em treino, teste e validação\n",
    "        # utilize as imagens nos arquivos train1.txt e val1.txt como treino\n",
    "        # utilize as imagens nos arquivos teste1.txt como validação\n",
    "        # lembre-se de atribuir o label de acordo com o dicionário self.le definido acima\n",
    "\n",
    "        img_list, labels = [], []\n",
    "\n",
    "        #...\n",
    "\n",
    "        return img_list, labels\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # IMPLEMENTE AQUI O RETORNO E TRATAMENTO DE CADA IMAGEM\n",
    "\n",
    "        # lembre-se de aplicar as transformações enviadas ao dataloader (principalmente o ToTensor)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        # normaliza uma imagem\n",
    "        # se calc_norm for True, normaliza pela subtração da média dividida pelo desvio para cada canal da imagem\n",
    "        # se calc_norm for False, normaliza pelos valores pré-definidos de média e desvio padrão\n",
    "        if self.calc_norm is True:\n",
    "            for i in range(img.shape[0]):\n",
    "                mu = img[i, :, :].mean()\n",
    "                std = img[i, :, :].std()\n",
    "                img[i, :, :] = ((img[i, :, :] - mu) / std)\n",
    "        else:\n",
    "            img = torchvision.transforms.functional.normalize(img,\n",
    "                                                mean=torch.Tensor([0.485, 0.456, 0.406]),\n",
    "                                                std=torch.Tensor([0.229, 0.224, 0.225]))\n",
    "        return img\n",
    "\n",
    "\n",
    "def load_data(dataset, root, batch_size, resize=None):\n",
    "    # IMPLEMENTE AQUI A DEFINIÇÃO DAS TRANSFORMAÇÕES E DO DATALOADER\n",
    "\n",
    "    # o transformer define a sequência de transformações que serão aplicadas na imagem\n",
    "    # a principal para o nosso caso é o ToTensor, que converte a imagem no formato lido para um tensor\n",
    "    # experimente transformações diferentes, como crops e flips\n",
    "    # o resize pode ser necessário para datasets com imagems de tamanhos variados\n",
    "\n",
    "    # defina também o dataloader de treino e teste\n",
    "\n",
    "    return train_iter, test_iter\n",
    "\n",
    "# carregamento do dado\n",
    "batch_size = 32\n",
    "train_iter, test_iter = load_data(TextureDataset, os.path.join('dtd'), batch_size, resize=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bY5x343ZyuAu"
   },
   "outputs": [],
   "source": [
    "# IMPLEMENTE AQUI SUA REDE E DEFINIÇÃO DE LOSS E OTIMIZADOR\n",
    "\n",
    "# experimente criar redes do zero com o conhecimento adquirido no curso até agora\n",
    "# experimente também replicar redes já estabelecidas (alexnet, lenet, vgg e etc)\n",
    "# experimente também utilizar as redes pré-treinadas já implementadas no torchvision\n",
    "# para o caso de rede pré-treinada, lembre-se de modificar a saída da rede para o número de classes do problema\n",
    "\n",
    "# treinamento e validação\n",
    "train_validate(model, train_iter, test_iter, batch_size, optimizer, loss, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTrCVEiiekAy"
   },
   "outputs": [],
   "source": [
    "# função auxiliar para plotar imagens do dataset\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):\n",
    "    \"\"\"Plot a list of images.\"\"\"\n",
    "    figsize = (num_cols * scale, num_rows * scale)\n",
    "    axes = plt.subplots(num_rows, num_cols, figsize=figsize)[1].flatten()\n",
    "    for i, (ax, img) in enumerate(zip(axes, imgs)):\n",
    "        ax.imshow(img.numpy())\n",
    "        ax.axes.get_xaxis().set_visible(False)\n",
    "        ax.axes.get_yaxis().set_visible(False)\n",
    "        if titles:\n",
    "            ax.set_title(titles[i])\n",
    "    return axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OllvZWX6epl9"
   },
   "outputs": [],
   "source": [
    "#plota imagens do dataset\n",
    "\n",
    "imgs = []\n",
    "for X, y in train_iter:\n",
    "    X = np.swapaxes(X, 1, 3)\n",
    "    imgs = X[0:18]\n",
    "    break\n",
    "\n",
    "\n",
    "show_images(imgs, 3, 6, titles=None, scale=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTihfBqEerPE"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "03GGRGweYCZm",
    "O59tc-hAdOm5",
    "S_26IvT9d5nF"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
