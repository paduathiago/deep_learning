{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flaviovdf/deep-ufmg/blob/main/listas/03-Redes-Neurais-em-PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NG-mVsVuE0if"
      },
      "source": [
        "# Preâmbulo\n",
        "\n",
        "Imports, funções, downloads e instalação do Pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEHmMCjR4PJw"
      },
      "source": [
        "# Basic imports.\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils import data\n",
        "from torch.backends import cudnn\n",
        "\n",
        "from torchvision import models\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "\n",
        "from skimage import io\n",
        "\n",
        "from sklearn import metrics\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "cudnn.benchmark = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQa4-lUw7Rmp"
      },
      "source": [
        "## Casting para o dispositivo correto\n",
        "\n",
        "Como usaremos processamento vetorial principalmente em GPUs para aprendizado profundo, primeiramente é possível verificar se há uma GPU disponível com o trecho de código abaixo, armazenando os tensores nos dispositivos apropriados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX0bBEM863sY"
      },
      "source": [
        "# Checking if GPU/CUDA is available.\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda')\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x5UK0uib2tk"
      },
      "source": [
        "# Intro MLP\n",
        "\n",
        "## Neurônios e a camada `nn.Linear`\n",
        "\n",
        "A camada Linear do Pytorch ([nn.Linear](https://pytorch.org/docs/stable/nn.html#torch.nn.Linear)) é responsável por aplicar uma transformação linear no dado de entrada. Esta camada recebe como parâmetro a dimensão (número de *features*) da entrada e da saída (que na verdade, representa o número de neurônios dessa camada). Por padrão o bias já é incluído. **Um** perceptron pode ser facilmente representado como a seguir, desconsiderando a função de ativação:\n",
        "\n",
        "```\n",
        "linear = nn.Linear(in_dimension, 1)\n",
        "```\n",
        "Mas de uma forma geral, uma camada Linear com diversas *features* de entrada e diversas *features* de saída pode ser representada como:\n",
        "```\n",
        "nn.Linear(in_features, out_features)\n",
        "```\n",
        "![](https://github.com/flaviovdf/deep-ufmg/blob/main/listas/figs/nn_linear.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlQA_vtGg8bf"
      },
      "source": [
        "linear = nn.Linear(2, 1)\n",
        "print(linear)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AhNyLrLmFcT"
      },
      "source": [
        "Como é possível ver no código abaixo, o Pytorch já inicia os pesos da camada aleatoriamente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLOlOhQVmPuj"
      },
      "source": [
        "for param in linear.parameters():\n",
        "  print(param)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAEaJGtDoZD7"
      },
      "source": [
        "O **forward** consiste em passar seu dado de entrada pela rede, gerando um resultado ao final. Considerando a camada linear instanciada anteriormente, o resultado do forward é o mesmo do somatório da multiplicação de seus pesos pelas respectivas entradas juntamente com o bias, ou seja:\n",
        "\n",
        "$$f_w(x) = w_0 + w_1x_1 + w_2x_2 + \\ldots + w_nx_n$$\n",
        "\n",
        "No Pytorch, realizamos o **forward** chamando o objeto onde nossa rede/modelo está instanciada, conforme exemplo abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ibb8t7zpmpUI"
      },
      "source": [
        "linear = nn.Linear(2, 1)\n",
        "X = torch.tensor([2.0, 3.0]) # dado de entrada de exemplo considerando o perceptron definido como nn.Linear(2,1)\n",
        "print('Pytorch: ', linear(X))\n",
        "\n",
        "# acessamos os pesos do modelo com .weight e o bias com .bias\n",
        "print('Manual: ', torch.sum(X * linear.weight) + linear.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ao fazermos linear(X), estamos fazendo implicitamente uma chamada na função forward da Linear\n",
        "print('Forward com chamada implícita:', linear(X))\n",
        "print('Forward com chamada explícita:', linear.forward(X))"
      ],
      "metadata": {
        "id": "zwjZ4t_MIz58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LMX6OMrEbw"
      },
      "source": [
        "## Exemplo de uma rede neural simples (1 camada)\n",
        "\n",
        "O código abaixo cria uma rede neural simples usando `nn.Linear` e implementa o fluxo de treinamento para essa rede, ou seja, faz o forward, calcula a loss, e otimiza seus pesos. Invista um pouco de tempo para entender a célula abaixo pois usaremos essa ideia para implementar a função de treino mais a frente."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iremos criar dados que seguem a função y = 2x + 3\n",
        "dataset = []\n",
        "for x in range(10):\n",
        "    dataset.append((x, 2*x + 3)) # tupla com (x, y)"
      ],
      "metadata": {
        "id": "_uGksRlgJYNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "Q2MwrrGAmFMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definiremos uma loss (erro quadrático -> (y - y_hat)^2)\n",
        "def loss_fn(predict, label):\n",
        "    return torch.pow(label - predict, 2)"
      ],
      "metadata": {
        "id": "UHRwAuyFJvUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPJ9s5ckoA5T"
      },
      "source": [
        "linear = nn.Linear(1, 1) # Camada linear com 1 feature de entrada (mais o bias) e uma de saída\n",
        "linear.to(device) # casting do linear para GPU\n",
        "\n",
        "learning_rate = 0.01\n",
        "print(f'Parâmetros iniciais: {list(linear.parameters())}\\n')\n",
        "\n",
        "for epoch in range(100):\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for X, y in dataset:\n",
        "        # Fazendo o casting dos dados para tensores na GPU\n",
        "        X = torch.FloatTensor([X]).to(device)\n",
        "        y = torch.FloatTensor([y]).to(device)\n",
        "\n",
        "        y_pred = linear(X)  # etapa de forward\n",
        "        loss = loss_fn(y_pred, y)  # calcula a loss\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Etapa de backprop\n",
        "        loss.backward()\n",
        "        with torch.no_grad(): # não queremos fazer com que o pytorch anote as operações do backprop e acumule o gradiente\n",
        "            for param in linear.parameters():\n",
        "                param -= learning_rate * param.grad  # atualização dos parametros (pesos e bias) com base no gradiente\n",
        "                param.grad.zero_()  # resetando o gradiente\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(\"Epoch {} - loss: {}\".format(epoch + 1, epoch_loss))\n",
        "\n",
        "print('\\nParâmetros finais: ', list(linear.parameters()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9uRk8mAwGIB"
      },
      "source": [
        "X = torch.FloatTensor([20]).to(device)\n",
        "print(linear(X)) # forward do valor 20 para conferir resultado, saida deve ser aproximadamente = 2x+3 = 2*20+3 = 43"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f-q6bCNzEmx"
      },
      "source": [
        "## O módulo `nn.Sequential`\n",
        "\n",
        "Na prática, criaremos redes com diversas camadas. O bloco `nn.Sequential` permite agrupar as camadas de forma sequencial para que o forward seja realizado na ordem desejada. Veja um exemplo para um *Multilayer Perceptron (MLP)* abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCK_OkqCzdUW"
      },
      "source": [
        "in_features = 28\n",
        "out_features = 8\n",
        "\n",
        "first_hidden_size = 64\n",
        "second_hidden_size = 32\n",
        "\n",
        "MLP = nn.Sequential(\n",
        "    nn.Linear(in_features, first_hidden_size), nn.ReLU(),\n",
        "    nn.Linear(first_hidden_size, second_hidden_size), nn.ReLU(),\n",
        "    nn.Linear(second_hidden_size, out_features)\n",
        ")\n",
        "\n",
        "print(MLP)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkhjCepK0kjJ"
      },
      "source": [
        "test_data = torch.randn((10, 28)) # 10 dados de input aleatórios com 28 features\n",
        "\n",
        "output = MLP(test_data) # forward da rede\n",
        "print(output.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb4zBjO0Lua"
      },
      "source": [
        "Informação sobre outras camadas lineares, como nn.Bilinear e nn.Identity, podem ser vistas na documentação: https://pytorch.org/docs/stable/nn.html#linear-layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9LoXL0cUYMT"
      },
      "source": [
        "## Conjunto de Exercícios 1 - Implementação de uma MLP\n",
        "\n",
        "Vamos agora treinar um MLP simples no dataset de [Breast Cancer da UCI](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)). A célula abaixo irá carregar os dados utilizando a biblioteca scikit-learn, que já possui o dataset pronto para utilizarmos.\n",
        "\n",
        "- Neste exercício, não estamos interessados em trabalhar com dados de treino/teste, mas sim apenas estudar como definir uma rede neural de múltiplas camadas em PyTorch e realizar o treinamento dos seus pesos e viéses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vi3Zh8fQ4X_3"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "\n",
        "# Convertendo os dados para tensores PyTorch e fazendo o casting para o dispositivo apropriado\n",
        "X = torch.FloatTensor(X).to(device)\n",
        "y = torch.FloatTensor(y).to(device)\n",
        "\n",
        "# Modificando o shape das anotações para ser um vetor (n, 1) para não gerar erros no cálculo da função de perda\n",
        "# Isso é necessário pois os produtos internos feitos no PyTorch irá resultar em uma matriz (n, 1), ou seja, n-linhas\n",
        "# onde cada linha terá um valor binário (classe daquela observação)\n",
        "y = y.reshape(y.shape[0], 1)\n",
        "\n",
        "# Exibindo o tamanho dos tensores\n",
        "print('Tamanho de X: ', X.size())\n",
        "print('Tamanho de y: ', y.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Labels dos dados:', torch.unique(y))"
      ],
      "metadata": {
        "id": "EjahTNqATUvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMDN1viW-0Eg"
      },
      "source": [
        "1. Implemente na célula abaixo uma MLP, de nome **model**, utilizando o módulo `nn.Sequential`. A sua rede MLP deve possuir, pelo menos, uma camada escondida, usando uma ReLU como função de ativação entre as camadas.\n",
        "\n",
        "    - Note que o nosso problema é um problema de classificação binária. Iremos utilizar uma função de perda do PyTorch que recebe como entrada a saída \"crua\" (*logits*) da sua rede neural. Sendo assim, não se preocupe em aplicar alguma transformação na saída (como uma sigmóide) para converter os valores para uma probabilidade, por exemplo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYO7HWC29Ahy"
      },
      "source": [
        "model = nn.Sequential(\n",
        "    # implemente aqui a sua solução!\n",
        ")\n",
        "\n",
        "model.to(device)  # sempre é necessario fazer o casting da rede para jogá-la para GPU\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWrn_MUHBz7o"
      },
      "source": [
        "Abaixo, definimos uma loss e um otimizador usando o PyTorch. Não se preocupem como isso funciona agora, pois iremos ver em detalhes como definir e usar diferentes losses e otimizadores com o PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Iremos utilizar a entropia cruzada binária como função de perda para o nosso problema\n",
        "# Essa versão da BCE aceita como entrada a saída \"crua\" (logits) da sua rede neural.\n",
        "# Outras versões, como a BCELoss aceita como entrada uma saída probabilística da sua rede neural (sigmoid ou softmax),\n",
        "# sendo assim você deve colocar uma função de ativação depois do último nn.Linear do seu modelo.\n",
        "# A versão BCEWithLogitsLoss implementa uma versão mais numericamente estável da loss, podemos observar isso na própria\n",
        "# documentação do PyTorch: https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html?highlight=bcewithlogitsloss#torch.nn.BCEWithLogitsLoss\n",
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "IgEeodMiSmuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa5DcYBf82iD"
      },
      "source": [
        "# Iremos utilizar o pacote optim para definir um otimizador que irá atualizar os pesos do modelo para nós.\n",
        "# Aqui, utilizaremos SGD - Gradiente Descendente Estocástico.\n",
        "# O pacote optim contém muitos outros algoritmos de otimização, porém, em todos o primeiro parâmetro irá dizer para os\n",
        "# otimizadores quais tensores (com requires_grad=True) do nosso modelo ele deverá otimizar.\n",
        "import torch.optim as optim\n",
        "\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPQtOnNr-kAG"
      },
      "source": [
        "Abaixo teremos um loop de treinamento típico de PyTorch. Não precisa modificar em nada essa funçao, porém estude ela mesmo assim, já que usaremos esse fluxo como template para treinamento de outros modelos mais a frente no curso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsMFRIDv80I3"
      },
      "source": [
        "# Aqui iremos criar uma lista de loss para cada época\n",
        "loss_list = []\n",
        "\n",
        "# Iterando sobre as épocas\n",
        "n_epochs = 500\n",
        "for epoch in range(n_epochs):\n",
        "    preds = model(X)\n",
        "    loss = criterion(preds, y)\n",
        "\n",
        "    # Salvando a loss da iteração atual (para plots futuros)\n",
        "    loss_list.append(loss.item())\n",
        "\n",
        "    # Antes de fazermos o backward pass, iremos zerar o gradiente de todos os tensores\n",
        "    # atrelados ao otimizador utilizando a chamada de função .zero_grad() do nosso otimizador.\n",
        "    # Faremos isso pois os gradientes são acumulados, sempre que chamamos .backward(), em buffers nos\n",
        "    # tensores que representam os pesos dos nossos modelos, ou seja, não são sobrescritos.\n",
        "    # Para mais detalhes, você pode dar uma olhada na documentação do torch.autograd.backward\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Realizando o backward pass, ou seja, computando os gradientes da nossa função de perda\n",
        "    # com respeito aos parâmetros (pesos) do nosso modelo\n",
        "    loss.backward()\n",
        "\n",
        "    # Chamando a função .step() do nosso otimizador para realizar um \"passo\" na otimização.\n",
        "    # Nesse caso, o \"passo\" será realizar o cálculo que vimos do gradiente descendente\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch {epoch + 1}: loss = {loss.item():.5f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toQyqq98-68X"
      },
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
        "\n",
        "ax.set_title('Evolução da função de perda ao longo das épocas')\n",
        "ax.set_ylabel('Valor da função de perda')\n",
        "ax.set_xlabel('Épocas')\n",
        "ax.plot(np.asarray(loss_list))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4aU4-sp5abBh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}