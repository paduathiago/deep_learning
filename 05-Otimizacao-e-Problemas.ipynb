{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed03SC1Jm9Yy"
      },
      "source": [
        "# Problemas\n",
        "\n",
        "Como vimos acima, há muitos passos na criação e definição de uma nova rede neural.\n",
        "A grande parte desses ajustes dependem diretamente do problemas.\n",
        "\n",
        "Abaixo, listamos alguns problemas. Todos os problemas e datasets usados vem do [Center for Machine Learning and Intelligent Systems](http://archive.ics.uci.edu/ml/datasets.php).\n",
        "\n",
        "\n",
        "**Seu objetivo é determinar e implementar um modelo para cada problema.**\n",
        "\n",
        "Isso inclui:\n",
        "\n",
        "1. definir uma arquitetura.\n",
        "Por enquanto usando somente camadas [Lineares](https://pytorch.org/docs/stable/nn.html#linear), porém podemos variar as ativações, como [Sigmoid](https://pytorch.org/docs/stable/nn.html#sigmoid), [Tanh](https://pytorch.org/docs/stable/nn.html#tanh), [ReLU](https://pytorch.org/docs/stable/nn.html#relu), [LeakyReLU](https://pytorch.org/docs/stable/generated/torch.nn.LeakyReLU.html), [ELU](https://pytorch.org/docs/stable/generated/torch.nn.ELU.html), [SeLU](https://pytorch.org/docs/stable/generated/torch.nn.SELU.html), [PReLU](https://pytorch.org/docs/stable/generated/torch.nn.PReLU.html), [RReLU](https://pytorch.org/docs/stable/generated/torch.nn.RReLU.html)\n",
        "2. definir uma função de custo. Algums opções que vimos previamente incluem[L1](https://pytorch.org/docs/stable/nn.html#l1loss), [L2/MSE](https://pytorch.org/docs/stable/nn.html#mseloss), [Huber/SmoothL1](https://pytorch.org/docs/stable/nn.html#smoothl1loss), [*Cross-Entropy*](https://pytorch.org/docs/stable/nn.html#crossentropyloss), [Hinge](https://pytorch.org/docs/stable/nn.html#hingeembeddingloss)), e\n",
        "3. definir um algoritmo de otimização ([SGD](https://pytorch.org/docs/stable/optim.html#torch.optim.SGD), [RMSProp](https://pytorch.org/docs/stable/optim.html#torch.optim.RMSprop), [Adam](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam))\n",
        "\n",
        "A leitura do dado assim como a função de treinamento já estão implementados para você."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bzMRy-nFKua"
      },
      "source": [
        "# Preâmbulo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XW-VATPAldgt"
      },
      "outputs": [],
      "source": [
        "# imports basicos\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim, nn\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fs5RRCEpFKug"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<contextlib.ExitStack at 0x7ba58444fa30>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.ion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNofnRSOFKul",
        "outputId": "6ddf3fd8-db62-481a-b8a6-1cf64c084e22"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test if GPU is avaliable, if not, use cpu instead\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "n = torch.cuda.device_count()\n",
        "devices_ids = list(range(n))\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdHX3JaM_-7J"
      },
      "source": [
        "## Funções básicas\n",
        "\n",
        "Use a função `load_array ` declarada a seguir se voce ja tem os dados armazenados em um **array** (por exemplo um array do numpy, o `np.array`). Pode acontecer de que os nossos dados vêm simplesmente de um dataset que pode ser armazenado em um array, e portanto não é necessário fazer os outros passos mais complicados como carregar os dados do disco, etc; basta que possamos recuperar esses dados em *batches* aleatórios. O resultado dessa função é um `DataLoader` do Pytorch com os dados que fornecemos de entrada, e que permite que os acessamos da seguinte forma:\n",
        "\n",
        "```python\n",
        "data_loader = load_array(X, y, batch_size=32, is_train=True)\n",
        "for x_batch, y_batch in data_loader:\n",
        "    ### ... nossa iteração de treinamento aqui.\n",
        "```\n",
        "\n",
        "Essa função recebe como parâmetro os seguintes valores:\n",
        "\n",
        "- `features`: um array que contém as features de todas as instâncias do dataset. Por exemplo, no caso do MNIST seria um array de tamanho `(60000, 28, 28, 1)` com todas as imagens do dataset de treino.\n",
        "- `labels`: um array que contém os rótulos de cada instância de dados. No caso do MNIST, seria um array de tamanho `(60000,)` em que a posição `i` contém o rótulo do dígito da posição `i` do array `features`.\n",
        "- `batch_size`: tamanho do batch desejado\n",
        "- `is_train`: um booleano que indica se o dataset que estamos criando é o conjunto de treinamento ou não (conjunto de teste). A única mudança que isso causa no `Dataloader` resultante é que se for o conjunto de treinamento ele cria batches aleatórios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xAN7JCEPAEU4"
      },
      "outputs": [],
      "source": [
        "def load_array(features, labels, batch_size, is_train=True):\n",
        "    \"\"\"Construct a Torch data loader\"\"\"\n",
        "\n",
        "    ## transform the input arrays in a tensor in case they are not\n",
        "    if type(features) != torch.tensor:\n",
        "        features = torch.tensor(features)\n",
        "    if type(labels) != torch.tensor:\n",
        "        labels = torch.tensor(labels)\n",
        "\n",
        "    ## create a Pytorch Dataset and DataLoader with the input data\n",
        "    dataset = torch.utils.data.TensorDataset(features, labels)\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size, shuffle=is_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qn0Sh7KBLE0"
      },
      "source": [
        "Use a função `evaluate_accuracy` para calcular a acurácia e a *loss-function* para a rede em um conjunto de dados. Note que essa função pode ser usada tanto para avaliar a rede no conjunto de teste (no caso que usamos o `DataLoader` de teste) quanto o conjunto de treinamento (se usamos o `DataLoader` de treinamento). Os parâmetros são:\n",
        "- `data_iter`: um `DataLoader` que contém os dados que queremos usar para avaliar a rede. Repare que esse parâmetro tipicamente é o ojeto que obtemos como saída da função `load_array` para montar o nosso `DataLoader`.\n",
        "- `net`: a rede que queremos avaliar\n",
        "- `loss`: a nossa *loss-function*. Pode ser um objeto de qualquer uma das funções de perda que mencionamos acima no começo do notebook.\n",
        "\n",
        "O resultado dessa função é uma tupla em que o primeiro valor é a acurácia e o segundo a função de custo calculados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SRGaUQsEFH0g"
      },
      "outputs": [],
      "source": [
        "# Função usada para calcular acurácia\n",
        "def evaluate_accuracy(data_iter, net, loss):\n",
        "    \"\"\"Evaluate accuracy of a model on the given data set.\"\"\"\n",
        "\n",
        "    ## valores \"acumuladores\", que guardam a soma de, respectivamente, quantas instâncias\n",
        "    ## prevemos corretamente, quantas instâncias percorremos no dataset, e o valor da loss; para\n",
        "    ## todos os batches\n",
        "    acc_sum, n, l = 0, 0, 0\n",
        "\n",
        "    ## muda a rede para o \"modo de teste\". O que isso faz é mudar o comportamento de alguns módulos da rede,\n",
        "    ## como os módulos de Dropout e BatchNorm, que funcionam de forma diferente quando estamos treinando ou\n",
        "    ## quando estamos avaliando (ou usando em produção) a rede\n",
        "    net.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for X, y in data_iter:\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          y_hat = net(X)\n",
        "          l += loss(y_hat, y.long())\n",
        "\n",
        "          ## aqui estamos calculando a quantidade de previsões que temos correta para o batch atual. o resultado\n",
        "          ## do argmax é a posição de `y_hat` que possui o maior valor. Consequentemente isso resulta na classe que\n",
        "          ## a rede deu o maior score.\n",
        "          acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "\n",
        "          ##\n",
        "          n += y.size(0)\n",
        "\n",
        "    return acc_sum / n, l.item() / len(data_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBmoehobG-TC"
      },
      "source": [
        "A função `train_validate` é a função que implementa nossas iterações de treinamento padrão. Ela ja faz o trabalho de percorrer o dataset inteiro para cada época, e também de tempos em tempos avaliar a rede e mostar os resultados na tela. Para isso ela faz chamadas à função `evaluate_accuracy` declarada anteriormente (entre outras coisas). Essa função tem os segugintes parâmetros:\n",
        "- `net`: a rede que queremos treinar\n",
        "- `train_iter` e `test_iter`: nossos `DataLoaders` que criamos para acessar os dados. Esses DataLoaders podem ser criados com a função `load_array` declarada acima.\n",
        "- `trainer`: é o nosso otimizador. Podemos usar aqui qualquer um dos otimizadores que escolhermos da lista citada no começo desse notebook.\n",
        "- `loss`: a loss function que escolhemos para otimizar. Pode ser qualquer um das funções de custo citadas no começo do notebook.\n",
        "- `num_epochs`: a quantidade de épocas pelas quais queremos que o treinamento ocorra.\n",
        "- `type`: o tipo de tarefa que estamos lidando. Se for um problema de regressão, usamos `type='regression'`, e se for um problema de classificação, usamos `type='classification'`. Esse parâmetro é necessário para a função, por exemplo, saber quais métricas ele vai mostrar (acurácia, ou apenas o MSE, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8oSVf8u1Oi1m"
      },
      "outputs": [],
      "source": [
        "# Função usada no treinamento e validação da rede\n",
        "def train_validate(net, train_iter, test_iter, trainer, loss, num_epochs, type='regression'):\n",
        "    print('training on', device)\n",
        "    for epoch in range(num_epochs):\n",
        "        train_l_sum, train_acc_sum, n, start = 0.0, 0.0, 0, time.time()\n",
        "        for X, y in train_iter:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            trainer.zero_grad()\n",
        "            y_hat = net(X)\n",
        "            if type == 'regression':\n",
        "              l = loss(y_hat, y.float())\n",
        "            else:\n",
        "              l = loss(y_hat, y.long())\n",
        "            l.backward()\n",
        "            trainer.step()\n",
        "            train_l_sum += l.item()\n",
        "            train_acc_sum += (y_hat.argmax(axis=1) == y).sum().item()\n",
        "            n += y.size(0)\n",
        "        test_acc, test_loss = evaluate_accuracy(test_iter, net, loss)\n",
        "        if type == 'regression':\n",
        "          print('epoch %d, train loss %.4f, test loss %.4f, time %.1f sec'\n",
        "                % (epoch + 1, train_l_sum / len(train_iter), test_loss, time.time() - start))\n",
        "        else:\n",
        "          print('epoch %d, train loss %.4f, train acc %.3f, test loss %.4f, '\n",
        "              'test acc %.3f, time %.1f sec'\n",
        "              % (epoch + 1, train_l_sum / len(train_iter), train_acc_sum / n, test_loss,\n",
        "                 test_acc, time.time() - start))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExWzvYS9JqTC"
      },
      "source": [
        "Use a função a seguir para inicializar os pesos da rede. Ela recebe como parâmetro um módulo da rede neural, e se for uma camada linear ele inicializa os pesos e os bias dessa camada. Embora possa parecer complicado de precisar chamar essa função para todas as camadas lineares da nossa rede, o módulos do Pytorch (que incluem tanto as redes criadas com o `nn.Sequential` ou com `nn.Module`) possuem a função `net.apply()` que recebe como parâmetro uma função e aplica ela a todos os submódulos da rede. Portanto, depois de ter criado a nossa rede, podemos chamar:\n",
        "\n",
        "```python\n",
        "net.apply(weights_init)\n",
        "```\n",
        "que automaticamente todas as camadas `nn.Linear` serão inicializadas. Caso queira saber mais sobre o `.apply()`, veja o seguinte [link](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.apply)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mkeIXH1PJqpA"
      },
      "outputs": [],
      "source": [
        "# Função para inicializar pesos da rede\n",
        "def weights_init(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        m.weight.data.normal_(0.0, 0.01) # valores iniciais são uma normal\n",
        "        m.bias.data.fill_(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0m-qic-0Wnl"
      },
      "source": [
        "# Problema 1\n",
        "\n",
        "Neste problema, você receberá 14 *features* coletadas de pacientes e tentará predizer se eles tem algum sinal de doença cardíaca. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2V7qc5FNZpC",
        "outputId": "a3a2c486-9050-4024-df0d-6d10be616444"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-19 23:35:34--  https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘processed.cleveland.data’\n",
            "\n",
            "processed.cleveland     [ <=>                ]  18,03K  95,5KB/s    in 0,2s    \n",
            "\n",
            "2025-05-19 23:35:43 (95,5 KB/s) - ‘processed.cleveland.data’ saved [18461]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## aqui fazemos o download do dataset usando o `!wget`. Se estamos rodando em um servidor linux (como é o caso do Colab),\n",
        "## podemos usar comandos do linux precedidos pelo \"!\". Por exemplo podemos fazer !ls para listar os arquivos da instância do colab.\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n",
        "\n",
        "## aqui fazemos um tratamento inicial dos dados. \"np.genfromtxt\" lê os dados de um arquivo .txt e transforma em\n",
        "## um array. Pode ser interessante abrir o arquivo para verificar como os dados chegaram. Se estiver no colab, voce\n",
        "## pode verificar o arquivo \"processed.cleveland.data\" clicando na pastinha do canto esquerdo da página. a função\n",
        "## \"np.nan_to_num\" trata valores NaN e infinitos no dataset.\n",
        "data = np.genfromtxt('processed.cleveland.data', delimiter=',', dtype=np.float32)\n",
        "data = np.nan_to_num(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdIA4XYoNa_2",
        "outputId": "1ef9836f-11c0-4184-b72c-8275994a7aad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(303, 14) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6.    0. ]\n",
            "(303, 13) [ 63.    1.    1.  145.  233.    1.    2.  150.    0.    2.3   3.    0.\n",
            "   6. ]\n",
            "(303,) 0.0\n"
          ]
        }
      ],
      "source": [
        "## aqui separamos os dados entre features (X) e rótulo (y), e depois separamos em um conjunto de treinamento e teste\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "AUYOPZYH0Ztc"
      },
      "outputs": [],
      "source": [
        "## aqui criamos nossos DataLoaders para conseguirmos iterar nos dados\n",
        "batch_size = 64\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "178XNdRUpiQW"
      },
      "outputs": [],
      "source": [
        "class HeartDiseaseNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HeartDiseaseNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(13, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 5),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HeartDiseaseNet(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=32, out_features=5, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = HeartDiseaseNet().to(device)\n",
        "model.apply(weights_init)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 1.5948, train acc 0.471, test loss 1.5756, test acc 0.475, time 0.3 sec\n",
            "epoch 2, train loss 1.5380, train acc 0.558, test loss 1.5070, test acc 0.475, time 0.0 sec\n",
            "epoch 3, train loss 1.4359, train acc 0.558, test loss 1.4049, test acc 0.475, time 0.0 sec\n",
            "epoch 4, train loss 1.3110, train acc 0.558, test loss 1.4083, test acc 0.475, time 0.0 sec\n",
            "epoch 5, train loss 1.2652, train acc 0.558, test loss 1.4521, test acc 0.475, time 0.0 sec\n",
            "epoch 6, train loss 1.2800, train acc 0.558, test loss 1.4055, test acc 0.475, time 0.0 sec\n",
            "epoch 7, train loss 1.2623, train acc 0.558, test loss 1.3738, test acc 0.475, time 0.0 sec\n",
            "epoch 8, train loss 1.2701, train acc 0.558, test loss 1.3714, test acc 0.475, time 0.0 sec\n",
            "epoch 9, train loss 1.2586, train acc 0.558, test loss 1.3708, test acc 0.475, time 0.0 sec\n",
            "epoch 10, train loss 1.2391, train acc 0.558, test loss 1.3755, test acc 0.475, time 0.0 sec\n",
            "epoch 11, train loss 1.2426, train acc 0.558, test loss 1.3864, test acc 0.475, time 0.0 sec\n",
            "epoch 12, train loss 1.2417, train acc 0.558, test loss 1.3786, test acc 0.475, time 0.0 sec\n",
            "epoch 13, train loss 1.2504, train acc 0.558, test loss 1.3690, test acc 0.475, time 0.0 sec\n",
            "epoch 14, train loss 1.2264, train acc 0.558, test loss 1.3543, test acc 0.475, time 0.0 sec\n",
            "epoch 15, train loss 1.2366, train acc 0.558, test loss 1.3496, test acc 0.475, time 0.0 sec\n",
            "epoch 16, train loss 1.2389, train acc 0.558, test loss 1.3547, test acc 0.475, time 0.0 sec\n",
            "epoch 17, train loss 1.2247, train acc 0.558, test loss 1.3493, test acc 0.475, time 0.0 sec\n",
            "epoch 18, train loss 1.2169, train acc 0.558, test loss 1.3370, test acc 0.475, time 0.0 sec\n",
            "epoch 19, train loss 1.2286, train acc 0.558, test loss 1.3304, test acc 0.475, time 0.0 sec\n",
            "epoch 20, train loss 1.2181, train acc 0.558, test loss 1.3227, test acc 0.475, time 0.0 sec\n",
            "epoch 21, train loss 1.2010, train acc 0.558, test loss 1.3165, test acc 0.475, time 0.0 sec\n",
            "epoch 22, train loss 1.1977, train acc 0.558, test loss 1.3154, test acc 0.475, time 0.0 sec\n",
            "epoch 23, train loss 1.2003, train acc 0.558, test loss 1.3028, test acc 0.475, time 0.0 sec\n",
            "epoch 24, train loss 1.1852, train acc 0.558, test loss 1.2852, test acc 0.475, time 0.0 sec\n",
            "epoch 25, train loss 1.1810, train acc 0.558, test loss 1.2732, test acc 0.475, time 0.0 sec\n",
            "epoch 26, train loss 1.1747, train acc 0.558, test loss 1.2715, test acc 0.475, time 0.0 sec\n",
            "epoch 27, train loss 1.1693, train acc 0.558, test loss 1.2554, test acc 0.475, time 0.0 sec\n",
            "epoch 28, train loss 1.1450, train acc 0.558, test loss 1.2345, test acc 0.475, time 0.0 sec\n",
            "epoch 29, train loss 1.1484, train acc 0.558, test loss 1.2270, test acc 0.475, time 0.0 sec\n",
            "epoch 30, train loss 1.1248, train acc 0.558, test loss 1.2071, test acc 0.475, time 0.0 sec\n",
            "epoch 31, train loss 1.1161, train acc 0.558, test loss 1.2000, test acc 0.475, time 0.0 sec\n",
            "epoch 32, train loss 1.1135, train acc 0.558, test loss 1.1827, test acc 0.475, time 0.0 sec\n",
            "epoch 33, train loss 1.0983, train acc 0.558, test loss 1.1712, test acc 0.492, time 0.0 sec\n",
            "epoch 34, train loss 1.0889, train acc 0.562, test loss 1.1648, test acc 0.475, time 0.0 sec\n",
            "epoch 35, train loss 1.0908, train acc 0.583, test loss 1.1563, test acc 0.508, time 0.0 sec\n",
            "epoch 36, train loss 1.0753, train acc 0.574, test loss 1.1474, test acc 0.541, time 0.0 sec\n",
            "epoch 37, train loss 1.0680, train acc 0.595, test loss 1.1395, test acc 0.525, time 0.0 sec\n",
            "epoch 38, train loss 1.0523, train acc 0.583, test loss 1.1395, test acc 0.525, time 0.0 sec\n",
            "epoch 39, train loss 1.0613, train acc 0.591, test loss 1.1215, test acc 0.525, time 0.0 sec\n",
            "epoch 40, train loss 1.0677, train acc 0.579, test loss 1.1202, test acc 0.525, time 0.0 sec\n",
            "epoch 41, train loss 1.0634, train acc 0.595, test loss 1.1192, test acc 0.525, time 0.0 sec\n",
            "epoch 42, train loss 1.0341, train acc 0.583, test loss 1.1088, test acc 0.541, time 0.0 sec\n",
            "epoch 43, train loss 1.0587, train acc 0.599, test loss 1.1205, test acc 0.525, time 0.0 sec\n",
            "epoch 44, train loss 1.0190, train acc 0.599, test loss 1.0944, test acc 0.541, time 0.0 sec\n",
            "epoch 45, train loss 1.0152, train acc 0.603, test loss 1.0930, test acc 0.541, time 0.0 sec\n",
            "epoch 46, train loss 1.0164, train acc 0.599, test loss 1.0915, test acc 0.541, time 0.0 sec\n",
            "epoch 47, train loss 1.0053, train acc 0.616, test loss 1.0796, test acc 0.541, time 0.0 sec\n",
            "epoch 48, train loss 1.0225, train acc 0.612, test loss 1.0782, test acc 0.557, time 0.0 sec\n",
            "epoch 49, train loss 0.9970, train acc 0.612, test loss 1.0703, test acc 0.541, time 0.0 sec\n",
            "epoch 50, train loss 1.0076, train acc 0.607, test loss 1.0739, test acc 0.541, time 0.0 sec\n",
            "epoch 51, train loss 0.9966, train acc 0.616, test loss 1.0606, test acc 0.525, time 0.0 sec\n",
            "epoch 52, train loss 0.9870, train acc 0.612, test loss 1.0593, test acc 0.541, time 0.0 sec\n",
            "epoch 53, train loss 0.9802, train acc 0.624, test loss 1.0512, test acc 0.525, time 0.0 sec\n",
            "epoch 54, train loss 0.9688, train acc 0.624, test loss 1.0498, test acc 0.557, time 0.0 sec\n",
            "epoch 55, train loss 0.9692, train acc 0.616, test loss 1.0437, test acc 0.525, time 0.0 sec\n",
            "epoch 56, train loss 0.9676, train acc 0.620, test loss 1.0405, test acc 0.541, time 0.0 sec\n",
            "epoch 57, train loss 0.9630, train acc 0.624, test loss 1.0367, test acc 0.557, time 0.0 sec\n",
            "epoch 58, train loss 0.9545, train acc 0.620, test loss 1.0329, test acc 0.557, time 0.0 sec\n",
            "epoch 59, train loss 0.9558, train acc 0.628, test loss 1.0316, test acc 0.557, time 0.0 sec\n",
            "epoch 60, train loss 0.9579, train acc 0.616, test loss 1.0322, test acc 0.541, time 0.0 sec\n",
            "epoch 61, train loss 0.9508, train acc 0.624, test loss 1.0268, test acc 0.541, time 0.0 sec\n",
            "epoch 62, train loss 0.9544, train acc 0.612, test loss 1.0235, test acc 0.574, time 0.0 sec\n",
            "epoch 63, train loss 0.9422, train acc 0.624, test loss 1.0202, test acc 0.557, time 0.0 sec\n",
            "epoch 64, train loss 0.9431, train acc 0.624, test loss 1.0183, test acc 0.557, time 0.0 sec\n",
            "epoch 65, train loss 0.9420, train acc 0.620, test loss 1.0164, test acc 0.557, time 0.0 sec\n",
            "epoch 66, train loss 0.9493, train acc 0.620, test loss 1.0131, test acc 0.574, time 0.0 sec\n",
            "epoch 67, train loss 0.9505, train acc 0.616, test loss 1.0082, test acc 0.557, time 0.0 sec\n",
            "epoch 68, train loss 0.9187, train acc 0.616, test loss 1.0049, test acc 0.541, time 0.0 sec\n",
            "epoch 69, train loss 0.9211, train acc 0.620, test loss 1.0024, test acc 0.574, time 0.0 sec\n",
            "epoch 70, train loss 0.9307, train acc 0.624, test loss 1.0010, test acc 0.574, time 0.0 sec\n",
            "epoch 71, train loss 0.9255, train acc 0.624, test loss 0.9999, test acc 0.574, time 0.0 sec\n",
            "epoch 72, train loss 0.9187, train acc 0.624, test loss 0.9980, test acc 0.590, time 0.0 sec\n",
            "epoch 73, train loss 0.9151, train acc 0.620, test loss 1.0029, test acc 0.557, time 0.0 sec\n",
            "epoch 74, train loss 0.9159, train acc 0.620, test loss 0.9971, test acc 0.590, time 0.0 sec\n",
            "epoch 75, train loss 0.9043, train acc 0.628, test loss 0.9967, test acc 0.590, time 0.0 sec\n",
            "epoch 76, train loss 0.9048, train acc 0.620, test loss 0.9951, test acc 0.574, time 0.0 sec\n",
            "epoch 77, train loss 0.9080, train acc 0.616, test loss 0.9992, test acc 0.574, time 0.0 sec\n",
            "epoch 78, train loss 0.9089, train acc 0.628, test loss 0.9941, test acc 0.574, time 0.0 sec\n",
            "epoch 79, train loss 0.8980, train acc 0.632, test loss 0.9964, test acc 0.574, time 0.0 sec\n",
            "epoch 80, train loss 0.8960, train acc 0.636, test loss 0.9927, test acc 0.574, time 0.0 sec\n",
            "epoch 81, train loss 0.8983, train acc 0.632, test loss 0.9911, test acc 0.607, time 0.0 sec\n",
            "epoch 82, train loss 0.9067, train acc 0.628, test loss 0.9917, test acc 0.574, time 0.0 sec\n",
            "epoch 83, train loss 0.9008, train acc 0.624, test loss 0.9932, test acc 0.574, time 0.0 sec\n",
            "epoch 84, train loss 0.8906, train acc 0.628, test loss 0.9965, test acc 0.590, time 0.0 sec\n",
            "epoch 85, train loss 0.9024, train acc 0.628, test loss 0.9929, test acc 0.574, time 0.0 sec\n",
            "epoch 86, train loss 0.9064, train acc 0.632, test loss 0.9919, test acc 0.607, time 0.0 sec\n",
            "epoch 87, train loss 0.9133, train acc 0.620, test loss 0.9922, test acc 0.607, time 0.0 sec\n",
            "epoch 88, train loss 0.8841, train acc 0.632, test loss 0.9972, test acc 0.607, time 0.0 sec\n",
            "epoch 89, train loss 0.8903, train acc 0.645, test loss 0.9977, test acc 0.557, time 0.0 sec\n",
            "epoch 90, train loss 0.8873, train acc 0.632, test loss 0.9956, test acc 0.590, time 0.0 sec\n",
            "epoch 91, train loss 0.8933, train acc 0.632, test loss 0.9959, test acc 0.574, time 0.0 sec\n",
            "epoch 92, train loss 0.8930, train acc 0.632, test loss 1.0011, test acc 0.590, time 0.0 sec\n",
            "epoch 93, train loss 0.8858, train acc 0.632, test loss 1.0017, test acc 0.557, time 0.0 sec\n",
            "epoch 94, train loss 0.8987, train acc 0.624, test loss 0.9923, test acc 0.607, time 0.0 sec\n",
            "epoch 95, train loss 0.8852, train acc 0.624, test loss 0.9946, test acc 0.607, time 0.0 sec\n",
            "epoch 96, train loss 0.8935, train acc 0.620, test loss 0.9954, test acc 0.590, time 0.0 sec\n",
            "epoch 97, train loss 0.8939, train acc 0.632, test loss 0.9920, test acc 0.607, time 0.0 sec\n",
            "epoch 98, train loss 0.8885, train acc 0.636, test loss 0.9929, test acc 0.607, time 0.0 sec\n",
            "epoch 99, train loss 0.9303, train acc 0.620, test loss 0.9959, test acc 0.590, time 0.0 sec\n",
            "epoch 100, train loss 0.9255, train acc 0.628, test loss 0.9976, test acc 0.590, time 0.0 sec\n"
          ]
        }
      ],
      "source": [
        "train_validate(model, train_iter, test_iter, optimizer, criterion, num_epochs=100, type='classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDaRVNq1aMpm"
      },
      "source": [
        "# Problema 2\n",
        "\n",
        "Neste problema, você receberá 90 *features* extraídas de diversas músicas (datadas de 1922 até 2011) e deve predizer o ano de cada música. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWdBT3zhW_Y5",
        "outputId": "ea82128a-aa9a-49ce-abc9-3a4568ae4232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-19 23:36:47--  http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘YearPredictionMSD.txt.zip’\n",
            "\n",
            "YearPredictionMSD.t     [   <=>              ] 201,24M  3,42MB/s    in 49s     \n",
            "\n",
            "2025-05-19 23:37:37 (4,13 MB/s) - ‘YearPredictionMSD.txt.zip’ saved [211011981]\n",
            "\n",
            "Archive:  YearPredictionMSD.txt.zip\n",
            "  inflating: YearPredictionMSD.txt   \n",
            "[ 2.0010000e+03  4.9943569e+01  2.1471140e+01  7.3077499e+01\n",
            "  8.7486095e+00 -1.7406281e+01 -1.3099050e+01 -2.5012020e+01\n",
            " -1.2232570e+01  7.8308902e+00 -2.4678299e+00  3.3213601e+00\n",
            " -2.3152101e+00  1.0205560e+01  6.1110913e+02  9.5108960e+02\n",
            "  6.9811426e+02  4.0898486e+02  3.8370911e+02  3.2651511e+02\n",
            "  2.3811327e+02  2.5142413e+02  1.8717351e+02  1.0042652e+02\n",
            "  1.7919498e+02 -8.4155798e+00 -3.1787039e+02  9.5862663e+01\n",
            "  4.8102589e+01 -9.5663033e+01 -1.8062149e+01  1.9698400e+00\n",
            "  3.4424381e+01  1.1726700e+01  1.3679000e+00  7.7944398e+00\n",
            " -3.6994001e-01 -1.3367851e+02 -8.3261650e+01 -3.7297649e+01\n",
            "  7.3046669e+01 -3.7366840e+01 -3.1385300e+00 -2.4215309e+01\n",
            " -1.3230660e+01  1.5938090e+01 -1.8604780e+01  8.2154793e+01\n",
            "  2.4057980e+02 -1.0294070e+01  3.1584311e+01 -2.5381870e+01\n",
            " -3.9077201e+00  1.3292580e+01  4.1550598e+01 -7.2627201e+00\n",
            " -2.1008631e+01  1.0550848e+02  6.4298561e+01  2.6084810e+01\n",
            " -4.4591099e+01 -8.3065701e+00  7.9370599e+00 -1.0736600e+01\n",
            " -9.5447662e+01 -8.2033073e+01 -3.5591942e+01  4.6952500e+00\n",
            "  7.0956261e+01  2.8091391e+01  6.0201502e+00 -3.7137669e+01\n",
            " -4.1124500e+01 -8.4081602e+00  7.1987700e+00 -8.6017599e+00\n",
            " -5.9085698e+00 -1.2324370e+01  1.4687340e+01 -5.4321251e+01\n",
            "  4.0147861e+01  1.3016200e+01 -5.4405479e+01  5.8993671e+01\n",
            "  1.5373440e+01  1.1114399e+00 -2.3087931e+01  6.8407951e+01\n",
            " -1.8222300e+00 -2.7463480e+01  2.2632699e+00]\n"
          ]
        }
      ],
      "source": [
        "# download do dataset\n",
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/00203/YearPredictionMSD.txt.zip\n",
        "!unzip YearPredictionMSD.txt.zip\n",
        "data = np.genfromtxt('YearPredictionMSD.txt', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data[0, :])\n",
        "X, y = data[:, 1:], data[:, 0]\n",
        "train_features, test_features, train_labels, test_labels = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features, train_labels, batch_size)\n",
        "test_iter = load_array(test_features, test_labels, batch_size, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "VoHCfAjzft_3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thiago/UFMG/10_periodo/deep_learning/deep_learning/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/home/thiago/UFMG/10_periodo/deep_learning/deep_learning/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([81])) that is different to the input size (torch.Size([81, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/home/thiago/UFMG/10_periodo/deep_learning/deep_learning/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1, train loss 92139.9850, test loss 28725.0535, time 9.9 sec\n",
            "epoch 2, train loss 20459.4988, test loss 17078.7537, time 9.3 sec\n",
            "epoch 3, train loss 17727.5458, test loss 14588.3951, time 9.1 sec\n",
            "epoch 4, train loss 16045.7524, test loss 20276.7737, time 10.4 sec\n",
            "epoch 5, train loss 15132.9285, test loss 12680.7102, time 11.5 sec\n",
            "epoch 6, train loss 14015.1948, test loss 12705.3004, time 12.2 sec\n",
            "epoch 7, train loss 13555.5597, test loss 16629.1052, time 13.0 sec\n",
            "epoch 8, train loss 12971.2334, test loss 13823.1675, time 10.6 sec\n",
            "epoch 9, train loss 12409.2308, test loss 11535.8154, time 11.3 sec\n",
            "epoch 10, train loss 12042.3070, test loss 12340.0364, time 11.5 sec\n",
            "epoch 11, train loss 11690.8348, test loss 10617.5873, time 9.9 sec\n",
            "epoch 12, train loss 11289.9580, test loss 10643.3239, time 10.1 sec\n",
            "epoch 13, train loss 11075.4329, test loss 12171.4039, time 10.1 sec\n",
            "epoch 14, train loss 10754.5068, test loss 9933.9553, time 11.4 sec\n",
            "epoch 15, train loss 10451.7050, test loss 10588.8771, time 11.7 sec\n",
            "epoch 16, train loss 10165.4342, test loss 9446.0506, time 11.3 sec\n",
            "epoch 17, train loss 9825.4910, test loss 9441.7454, time 13.1 sec\n",
            "epoch 18, train loss 9464.3765, test loss 8411.3598, time 11.9 sec\n",
            "epoch 19, train loss 8811.0497, test loss 8555.3422, time 11.3 sec\n",
            "epoch 20, train loss 7823.5169, test loss 7149.3704, time 10.8 sec\n",
            "epoch 21, train loss 4565.3086, test loss 1272.1892, time 12.3 sec\n",
            "epoch 22, train loss 410.2481, test loss 165.8686, time 11.8 sec\n",
            "epoch 23, train loss 232.0030, test loss 174.1348, time 10.7 sec\n",
            "epoch 24, train loss 184.8692, test loss 157.4517, time 11.0 sec\n",
            "epoch 25, train loss 159.2700, test loss 134.3442, time 10.6 sec\n",
            "epoch 26, train loss 154.5662, test loss 133.3836, time 10.5 sec\n",
            "epoch 27, train loss 149.7165, test loss 132.2948, time 12.1 sec\n",
            "epoch 28, train loss 134.7062, test loss 138.4525, time 10.8 sec\n",
            "epoch 29, train loss 133.0135, test loss 123.1453, time 13.0 sec\n",
            "epoch 30, train loss 131.4532, test loss 124.7539, time 11.6 sec\n",
            "epoch 31, train loss 127.3290, test loss 124.7124, time 11.5 sec\n",
            "epoch 32, train loss 125.2636, test loss 124.0710, time 10.5 sec\n",
            "epoch 33, train loss 125.1903, test loss 129.7726, time 9.9 sec\n",
            "epoch 34, train loss 124.1657, test loss 124.2558, time 11.5 sec\n",
            "epoch 35, train loss 124.0758, test loss 128.5612, time 11.4 sec\n",
            "epoch 36, train loss 123.6474, test loss 126.6814, time 11.9 sec\n",
            "epoch 37, train loss 123.7618, test loss 121.0619, time 11.3 sec\n",
            "epoch 38, train loss 123.8428, test loss 123.0338, time 12.5 sec\n",
            "epoch 39, train loss 123.8099, test loss 138.0212, time 11.9 sec\n",
            "epoch 40, train loss 123.6983, test loss 140.9045, time 10.7 sec\n",
            "epoch 41, train loss 123.8156, test loss 119.9931, time 9.6 sec\n",
            "epoch 42, train loss 123.8325, test loss 122.5825, time 9.5 sec\n",
            "epoch 43, train loss 123.8827, test loss 120.6967, time 10.2 sec\n",
            "epoch 44, train loss 123.8841, test loss 134.5209, time 10.5 sec\n",
            "epoch 45, train loss 124.1557, test loss 125.9833, time 9.5 sec\n",
            "epoch 46, train loss 123.7863, test loss 133.3409, time 10.7 sec\n",
            "epoch 47, train loss 124.0620, test loss 120.0193, time 11.5 sec\n",
            "epoch 48, train loss 124.0529, test loss 126.1071, time 11.1 sec\n",
            "epoch 49, train loss 123.7194, test loss 125.1996, time 10.2 sec\n",
            "epoch 50, train loss 123.9260, test loss 120.0017, time 11.9 sec\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network architecture\n",
        "class MusicYearNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MusicYearNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(90, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model_2 = MusicYearNet().to(device)\n",
        "model_2.apply(weights_init)\n",
        "\n",
        "criterion_2 = nn.MSELoss()\n",
        "optimizer_2 = optim.Adam(model_2.parameters(), lr=0.001)\n",
        "\n",
        "train_validate(model_2, train_iter, test_iter, optimizer_2, criterion_2, num_epochs=50, type='regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNMAyyX8jSb8",
        "outputId": "74506ae1-a869-4351-c83c-57c4a8bce755"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1998.2603],\n",
            "        [1998.2603],\n",
            "        [1998.2603],\n",
            "        [1998.2603],\n",
            "        [1998.2603]], device='cuda:0', grad_fn=<AddmmBackward0>) [2008. 2001. 2006. 2008. 1998.]\n"
          ]
        }
      ],
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = model_2(torch.Tensor(test_features[0:5, :]).to(device))\n",
        "print(y, test_labels[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pd8hG7HCDUib"
      },
      "source": [
        "# Problema 3\n",
        "\n",
        "Neste problema, você receberá várias *features* (como altura média, inclinação, etc) descrevendo uma região e o modelo deve predizer qual o tipo da região (floresta, montanha, etc). Mais informações sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/covertype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZcIXGqBDznB",
        "outputId": "d33b284b-d5e7-416d-cb51-0541b559329a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-19 23:49:50--  http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘covtype.data.gz’\n",
            "\n",
            "covtype.data.gz         [               <=>  ]  10,72M  2,90MB/s    in 4,0s    \n",
            "\n",
            "2025-05-19 23:49:54 (2,65 MB/s) - ‘covtype.data.gz’ saved [11240707]\n",
            "\n",
            "gzip: covtype.data.gz already has .gz suffix -- unchanged\n",
            "(581012, 55) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 5.000e+00]\n",
            "(581012, 54) [2.596e+03 5.100e+01 3.000e+00 2.580e+02 0.000e+00 5.100e+02 2.210e+02\n",
            " 2.320e+02 1.480e+02 6.279e+03 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            " 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]\n",
            "(581012,) 5.0\n"
          ]
        }
      ],
      "source": [
        "!wget http://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz\n",
        "!gzip covtype.data.gz\n",
        "data = np.genfromtxt('covtype.data', delimiter=',', dtype=np.float32)\n",
        "\n",
        "print(data.shape, data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, X[0, :])\n",
        "print(y.shape, y[0])\n",
        "train_features_3, test_features_3, train_labels_3, test_labels_3 = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "train_labels_3 = train_labels_3 - 1\n",
        "test_labels_3 = test_labels_3 - 1\n",
        "\n",
        "batch_size = 100\n",
        "train_iter_3 = load_array(train_features_3, train_labels_3, batch_size)\n",
        "test_iter_3 = load_array(test_features_3, test_labels_3, batch_size, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5O0HJVIOZZW4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 0.7759, train acc 0.664, test loss 0.7032, test acc 0.694, time 13.0 sec\n",
            "epoch 2, train loss 0.6548, train acc 0.716, test loss 0.6413, test acc 0.731, time 12.6 sec\n",
            "epoch 3, train loss 0.6150, train acc 0.733, test loss 0.5831, test acc 0.750, time 13.8 sec\n",
            "epoch 4, train loss 0.5896, train acc 0.744, test loss 0.5842, test acc 0.742, time 12.6 sec\n",
            "epoch 5, train loss 0.5647, train acc 0.756, test loss 0.5507, test acc 0.767, time 13.6 sec\n",
            "epoch 6, train loss 0.5475, train acc 0.764, test loss 0.5686, test acc 0.756, time 13.5 sec\n",
            "epoch 7, train loss 0.5318, train acc 0.771, test loss 0.5139, test acc 0.778, time 13.4 sec\n",
            "epoch 8, train loss 0.5217, train acc 0.776, test loss 0.5097, test acc 0.782, time 11.7 sec\n",
            "epoch 9, train loss 0.5115, train acc 0.781, test loss 0.5183, test acc 0.778, time 11.6 sec\n",
            "epoch 10, train loss 0.5045, train acc 0.785, test loss 0.5197, test acc 0.772, time 11.7 sec\n",
            "epoch 11, train loss 0.4972, train acc 0.788, test loss 0.4834, test acc 0.793, time 11.7 sec\n",
            "epoch 12, train loss 0.4892, train acc 0.791, test loss 0.4884, test acc 0.790, time 11.6 sec\n",
            "epoch 13, train loss 0.4797, train acc 0.796, test loss 0.4887, test acc 0.791, time 11.5 sec\n",
            "epoch 14, train loss 0.4777, train acc 0.797, test loss 0.4741, test acc 0.797, time 11.5 sec\n",
            "epoch 15, train loss 0.4725, train acc 0.799, test loss 0.4718, test acc 0.799, time 11.6 sec\n",
            "epoch 16, train loss 0.4687, train acc 0.801, test loss 0.4582, test acc 0.807, time 11.8 sec\n",
            "epoch 17, train loss 0.4646, train acc 0.802, test loss 0.4576, test acc 0.803, time 11.6 sec\n",
            "epoch 18, train loss 0.4610, train acc 0.804, test loss 0.4563, test acc 0.806, time 11.6 sec\n",
            "epoch 19, train loss 0.4556, train acc 0.806, test loss 0.4534, test acc 0.807, time 13.0 sec\n",
            "epoch 20, train loss 0.4546, train acc 0.806, test loss 0.4613, test acc 0.805, time 14.9 sec\n",
            "epoch 21, train loss 0.4520, train acc 0.808, test loss 0.4598, test acc 0.808, time 15.8 sec\n",
            "epoch 22, train loss 0.4493, train acc 0.809, test loss 0.4552, test acc 0.807, time 14.9 sec\n",
            "epoch 23, train loss 0.4480, train acc 0.810, test loss 0.4508, test acc 0.809, time 15.2 sec\n",
            "epoch 24, train loss 0.4447, train acc 0.812, test loss 0.4431, test acc 0.813, time 15.0 sec\n",
            "epoch 25, train loss 0.4429, train acc 0.812, test loss 0.4467, test acc 0.811, time 14.0 sec\n",
            "epoch 26, train loss 0.4400, train acc 0.813, test loss 0.4292, test acc 0.818, time 13.2 sec\n",
            "epoch 27, train loss 0.4393, train acc 0.814, test loss 0.4413, test acc 0.815, time 13.7 sec\n",
            "epoch 28, train loss 0.4371, train acc 0.815, test loss 0.4653, test acc 0.802, time 12.4 sec\n",
            "epoch 29, train loss 0.4379, train acc 0.814, test loss 0.4302, test acc 0.817, time 12.5 sec\n",
            "epoch 30, train loss 0.4336, train acc 0.816, test loss 0.4402, test acc 0.815, time 14.0 sec\n",
            "epoch 31, train loss 0.4320, train acc 0.817, test loss 0.4398, test acc 0.812, time 12.0 sec\n",
            "epoch 32, train loss 0.4315, train acc 0.817, test loss 0.4247, test acc 0.819, time 11.6 sec\n",
            "epoch 33, train loss 0.4302, train acc 0.817, test loss 0.4289, test acc 0.818, time 11.6 sec\n",
            "epoch 34, train loss 0.4287, train acc 0.818, test loss 0.4270, test acc 0.819, time 11.7 sec\n",
            "epoch 35, train loss 0.4258, train acc 0.819, test loss 0.4472, test acc 0.805, time 11.7 sec\n",
            "epoch 36, train loss 0.4259, train acc 0.819, test loss 0.4140, test acc 0.825, time 12.6 sec\n",
            "epoch 37, train loss 0.4243, train acc 0.820, test loss 0.4361, test acc 0.815, time 13.3 sec\n",
            "epoch 38, train loss 0.4227, train acc 0.820, test loss 0.4232, test acc 0.821, time 12.6 sec\n",
            "epoch 39, train loss 0.4219, train acc 0.821, test loss 0.4303, test acc 0.817, time 13.0 sec\n",
            "epoch 40, train loss 0.4209, train acc 0.822, test loss 0.4429, test acc 0.809, time 14.7 sec\n",
            "epoch 41, train loss 0.4195, train acc 0.823, test loss 0.4347, test acc 0.813, time 14.1 sec\n",
            "epoch 42, train loss 0.4178, train acc 0.822, test loss 0.4242, test acc 0.822, time 13.8 sec\n",
            "epoch 43, train loss 0.4185, train acc 0.822, test loss 0.4293, test acc 0.820, time 14.9 sec\n",
            "epoch 44, train loss 0.4158, train acc 0.823, test loss 0.4117, test acc 0.827, time 14.5 sec\n",
            "epoch 45, train loss 0.4158, train acc 0.824, test loss 0.4303, test acc 0.817, time 14.2 sec\n",
            "epoch 46, train loss 0.4151, train acc 0.824, test loss 0.4076, test acc 0.828, time 14.2 sec\n",
            "epoch 47, train loss 0.4121, train acc 0.825, test loss 0.4407, test acc 0.814, time 12.7 sec\n",
            "epoch 48, train loss 0.4119, train acc 0.826, test loss 0.4156, test acc 0.825, time 12.6 sec\n",
            "epoch 49, train loss 0.4087, train acc 0.827, test loss 0.4279, test acc 0.819, time 12.5 sec\n",
            "epoch 50, train loss 0.4104, train acc 0.826, test loss 0.4296, test acc 0.816, time 12.6 sec\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network architecture\n",
        "class CoverTypeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CoverTypeNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(54, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 7),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model_3 = CoverTypeNet().to(device)\n",
        "model_3.apply(weights_init)\n",
        "\n",
        "criterion_3 = nn.CrossEntropyLoss()\n",
        "optimizer_3 = optim.Adam(model_3.parameters(), lr=0.001)\n",
        "\n",
        "train_validate(model_3, train_iter_3, test_iter_3, optimizer_3, criterion_3, num_epochs=50, type='classification')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 7.5493e+00,  1.1596e+00, -3.8146e+01, -3.5614e+02, -1.7936e+01,\n",
            "         -4.6732e+01,  4.0308e+00],\n",
            "        [-4.6020e-01,  2.4049e+00, -1.8511e+00, -3.1516e+01, -6.9974e-01,\n",
            "         -4.0119e+00, -1.2465e+01],\n",
            "        [-3.2605e+00,  2.1820e+00,  1.0269e+00, -9.0699e+00, -8.5586e-01,\n",
            "         -2.0352e+00, -1.4700e+01],\n",
            "        [ 1.9515e+00,  2.4313e+00, -2.1547e+01, -2.2555e+02, -2.6122e+00,\n",
            "         -1.8438e+01, -1.9116e+01],\n",
            "        [ 2.7471e-01,  3.4888e+00, -8.1679e+00, -9.8457e+01,  2.2671e-01,\n",
            "         -7.8202e+00, -3.5766e+01]], device='cuda:0', grad_fn=<AddmmBackward0>) [0. 1. 1. 1. 1.]\n"
          ]
        }
      ],
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = model_3(torch.Tensor(test_features_3[0:5, :]).to(device))\n",
        "print(y, test_labels_3[0:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOAxsRz0rpZl"
      },
      "source": [
        "# Problema 4\n",
        "\n",
        "Neste problema, você receberá 11 *features* extraídas de tipos de vinhos, e terá que predizer um *score* para cada vinho. Mais sobre esse dataset aqui: https://archive.ics.uci.edu/ml/datasets/Wine+Quality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "knTzA0O6rusi",
        "outputId": "e7e11b5c-b4aa-4b94-f699-c6d07bd60361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-05-20 00:05:24--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘winequality-red.csv.1’\n",
            "\n",
            "winequality-red.csv     [  <=>               ]  82,23K   223KB/s    in 0,4s    \n",
            "\n",
            "2025-05-20 00:05:25 (223 KB/s) - ‘winequality-red.csv.1’ saved [84199]\n",
            "\n",
            "--2025-05-20 00:05:25--  https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
            "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
            "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified\n",
            "Saving to: ‘winequality-white.csv.1’\n",
            "\n",
            "winequality-white.c     [   <=>              ] 258,23K   341KB/s    in 0,8s    \n",
            "\n",
            "2025-05-20 00:05:27 (341 KB/s) - ‘winequality-white.csv.1’ saved [264426]\n",
            "\n",
            "[ 7.4     0.7     0.      1.9     0.076  11.     34.      0.9978  3.51\n",
            "  0.56    9.4     5.    ]\n",
            "(6497, 11) (6497,)\n"
          ]
        }
      ],
      "source": [
        "# download do dataset\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\n",
        "!wget https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n",
        "data_red = np.genfromtxt('winequality-red.csv', delimiter=';', dtype=np.float32, skip_header=1)\n",
        "data_white = np.genfromtxt('winequality-white.csv', delimiter=';', dtype=np.float32, skip_header=1)\n",
        "data = np.concatenate((data_red, data_white), axis=0)\n",
        "data = np.nan_to_num(data)\n",
        "\n",
        "print(data[0, :])\n",
        "X, y = data[:, :-1], data[:, -1]\n",
        "print(X.shape, y.shape)\n",
        "train_features_4, test_features_4, train_labels_4, test_labels_4 = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "\n",
        "batch_size = 100\n",
        "train_iter = load_array(train_features_4, train_labels_4, batch_size)\n",
        "test_iter = load_array(test_features_4, test_labels_4, batch_size, False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9E7pwns4rx9l"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training on cuda\n",
            "epoch 1, train loss 19.6759, test loss 5.9013, time 0.2 sec\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thiago/UFMG/10_periodo/deep_learning/deep_learning/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([100])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/home/thiago/UFMG/10_periodo/deep_learning/deep_learning/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([52])) that is different to the input size (torch.Size([52, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n",
            "/home/thiago/UFMG/10_periodo/deep_learning/deep_learning/.venv/lib/python3.10/site-packages/torch/nn/modules/loss.py:610: UserWarning: Using a target size (torch.Size([45])) that is different to the input size (torch.Size([45, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2, train loss 4.8493, test loss 4.1678, time 0.2 sec\n",
            "epoch 3, train loss 3.2795, test loss 2.4743, time 0.1 sec\n",
            "epoch 4, train loss 1.6250, test loss 0.9955, time 0.1 sec\n",
            "epoch 5, train loss 0.9159, test loss 0.8656, time 0.1 sec\n",
            "epoch 6, train loss 0.8865, test loss 0.8533, time 0.1 sec\n",
            "epoch 7, train loss 0.8844, test loss 0.8490, time 0.1 sec\n",
            "epoch 8, train loss 0.8794, test loss 0.8507, time 0.1 sec\n",
            "epoch 9, train loss 0.8798, test loss 0.8428, time 0.1 sec\n",
            "epoch 10, train loss 0.8738, test loss 0.8399, time 0.2 sec\n",
            "epoch 11, train loss 0.8739, test loss 0.8400, time 0.1 sec\n",
            "epoch 12, train loss 0.8728, test loss 0.8375, time 0.1 sec\n",
            "epoch 13, train loss 0.8677, test loss 0.8335, time 0.1 sec\n",
            "epoch 14, train loss 0.8697, test loss 0.8423, time 0.1 sec\n",
            "epoch 15, train loss 0.8624, test loss 0.8392, time 0.1 sec\n",
            "epoch 16, train loss 0.8648, test loss 0.8260, time 0.2 sec\n",
            "epoch 17, train loss 0.8739, test loss 0.8267, time 0.1 sec\n",
            "epoch 18, train loss 0.8895, test loss 0.8723, time 0.2 sec\n",
            "epoch 19, train loss 0.8698, test loss 0.8457, time 0.1 sec\n",
            "epoch 20, train loss 0.8665, test loss 0.8401, time 0.1 sec\n",
            "epoch 21, train loss 0.8544, test loss 0.8242, time 0.1 sec\n",
            "epoch 22, train loss 0.8530, test loss 0.8206, time 0.1 sec\n",
            "epoch 23, train loss 0.8541, test loss 0.8150, time 0.1 sec\n",
            "epoch 24, train loss 0.8512, test loss 0.8127, time 0.1 sec\n",
            "epoch 25, train loss 0.8523, test loss 0.8181, time 0.1 sec\n",
            "epoch 26, train loss 0.8530, test loss 0.8105, time 0.1 sec\n",
            "epoch 27, train loss 0.8558, test loss 0.8244, time 0.1 sec\n",
            "epoch 28, train loss 0.8438, test loss 0.8097, time 0.1 sec\n",
            "epoch 29, train loss 0.8432, test loss 0.8107, time 0.1 sec\n",
            "epoch 30, train loss 0.8471, test loss 0.8152, time 0.1 sec\n",
            "epoch 31, train loss 0.8397, test loss 0.8032, time 0.1 sec\n",
            "epoch 32, train loss 0.8431, test loss 0.8026, time 0.2 sec\n",
            "epoch 33, train loss 0.8416, test loss 0.7989, time 0.1 sec\n",
            "epoch 34, train loss 0.8496, test loss 0.8362, time 0.1 sec\n",
            "epoch 35, train loss 0.8487, test loss 0.7967, time 0.1 sec\n",
            "epoch 36, train loss 0.8406, test loss 0.8071, time 0.1 sec\n",
            "epoch 37, train loss 0.8511, test loss 0.8117, time 0.1 sec\n",
            "epoch 38, train loss 0.8321, test loss 0.8003, time 0.1 sec\n",
            "epoch 39, train loss 0.8344, test loss 0.8113, time 0.1 sec\n",
            "epoch 40, train loss 0.8311, test loss 0.7956, time 0.1 sec\n",
            "epoch 41, train loss 0.8395, test loss 0.7925, time 0.1 sec\n",
            "epoch 42, train loss 0.8329, test loss 0.8121, time 0.1 sec\n",
            "epoch 43, train loss 0.8406, test loss 0.8025, time 0.1 sec\n",
            "epoch 44, train loss 0.8282, test loss 0.8072, time 0.1 sec\n",
            "epoch 45, train loss 0.8578, test loss 0.7969, time 0.1 sec\n",
            "epoch 46, train loss 0.8247, test loss 0.7855, time 0.1 sec\n",
            "epoch 47, train loss 0.8259, test loss 0.7846, time 0.2 sec\n",
            "epoch 48, train loss 0.8242, test loss 0.7954, time 0.1 sec\n",
            "epoch 49, train loss 0.8311, test loss 0.7828, time 0.1 sec\n",
            "epoch 50, train loss 0.8253, test loss 0.7855, time 0.1 sec\n"
          ]
        }
      ],
      "source": [
        "class WineQualityNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(WineQualityNet, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(11, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model_4 = WineQualityNet().to(device)\n",
        "model_4.apply(weights_init)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model_4.parameters(), lr=0.001)\n",
        "\n",
        "train_validate(model_4, train_iter, test_iter, optimizer, criterion, num_epochs=50, type='regression')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-At99Iqzryg8",
        "outputId": "83436484-3c68-4973-dde8-191f9453824a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[6.0086],\n",
            "        [5.8021],\n",
            "        [5.9521],\n",
            "        [5.5954],\n",
            "        [5.7929]], device='cuda:0', grad_fn=<AddmmBackward0>) [8. 5. 7. 6. 6.]\n"
          ]
        }
      ],
      "source": [
        "# mostra o resultado predito para as 5 primeiras instâncias de teste\n",
        "y = model_4(torch.Tensor(test_features_4[0:5, :]).to(device))\n",
        "print(y, test_labels_4[0:5])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
